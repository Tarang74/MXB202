%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\divergence}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\sign}{sgn}

% Header and footer
\newcommand{\unitName}{Advanced Calculus}
\newcommand{\unitTime}{Semester 2, 2023}
\newcommand{\unitCoordinator}{Dr Pascal Buenzli}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Euclidean Space}
The Euclidean space \(\R^n\) is an \(n\)-dimensional vector space of
real numbers. This space is closed under addition and scalar
multiplication.
\subsection{Operations}
\subsubsection{Addition}
The sum of two vectors \(\symbf{x}\) and \(\symbf{y}\) is defined
element-wise
\begin{equation*}
    \symbf{x} + \symbf{y} =
    \begin{bmatrix}
        x_1 + y_1 \\
        x_2 + y_2 \\
        \vdots    \\
        x_n + y_n
    \end{bmatrix}
\end{equation*}
In a coordinate system, the vectors \(\symbf{x}\) and \(\symbf{y}\) are added tip-to-tail.
\subsubsection{Scalar Multiplication}
The scalar multiplication of a vector \(\symbf{x}\) by a scalar
\(\lambda \in \R\) is defined element-wise
\begin{equation*}
    \lambda \symbf{x} =
    \begin{bmatrix}
        \lambda x_1 \\
        \lambda x_2 \\
        \vdots      \\
        \lambda x_n
    \end{bmatrix}
\end{equation*}
In a coordinate system, \(\lambda\) scales the vector \(\symbf{x}\) along the same line.
\subsubsection{Norm}
The norm (length) of a vector \(\symbf{x}\) is defined as
\begin{equation*}
    \norm{\symbf{x}} = \sqrt{\symbf{x} \cdot \symbf{x}} = \sqrt{\sum_{i=1}^n x_i^2}
\end{equation*}
The norm of a vector \(\symbf{x}\) is the distance from the origin to the tip of the vector.
This allows us to define the unit vector \(\hat{\symbf{x}}\) as
\begin{equation*}
    \hat{\symbf{x}} = \frac{\symbf{x}}{\norm{\symbf{x}}}
\end{equation*}
which is a vector of length 1 in the same direction as \(\symbf{x}\).
\subsubsection{Scalar Product}
The scalar product (dot product) of two vectors \(\symbf{x}\) and
\(\symbf{y}\) is defined as
\begin{equation*}
    \symbf{x} \cdot \symbf{y} = \sum_{i=1}^n x_i y_i
\end{equation*}
The scalar product allows us to define the angle \(\theta\) between two vectors \(\symbf{x}\) and \(\symbf{y}\) as
\begin{equation*}
    \cos{\left( \theta \right)} = \hat{\symbf{x}} \cdot \hat{\symbf{y}}
\end{equation*}
where we use the unit vectors of \(\symbf{x}\) and \(\symbf{y}\), as the angle between two vectors is invariant under scaling.
Additionally, we can determine the projection of the vector \(\symbf{x}\) onto the vector \(\symbf{y}\) using trigonometry
\begin{equation*}
    \proj_{\symbf{y}} \left( \symbf{x} \right) = \left( \norm{\symbf{x}} \cos{\left( \theta \right)} \right) \hat{\symbf{y}} = \left( \norm{\symbf{x}} \left( \hat{\symbf{x}} \cdot \hat{\symbf{y}} \right) \right) \hat{\symbf{y}} = \left( \symbf{x} \cdot \hat{\symbf{y}} \right) \hat{\symbf{y}}
\end{equation*}
where \(\symbf{x} \cdot \hat{\symbf{y}}\) is the norm of the projection vector.
\subsection{Normed Vector Space Axioms}
\subsubsection{Triangle Inequality}
\begin{equation*}
    \norm{\symbf{x} + \symbf{y}} \leqslant \norm{\symbf{x}} + \norm{\symbf{y}}
\end{equation*}
\subsubsection{Inverse Triangle Inequality}
\begin{equation*}
    \norm{\symbf{x} - \symbf{y}} \geqslant \abs{\norm{\symbf{x}} - \norm{\symbf{y}}}
\end{equation*}
\subsubsection{Cauchy-Schwarz Inequality}
\begin{equation*}
    \abs{\symbf{x} \cdot \symbf{y}} \leqslant \norm{\symbf{x}} \norm{\symbf{y}}
\end{equation*}
\subsection{Topological Properties}
\begin{definition}
    An open ball of radius \(r > 0\) centred at a
    point \(\symbf{p} \in \R^n\) is denoted \(B_r\left( \symbf{p}
    \right)\), and is defined as
    \begin{equation*}
        B_r\left( \symbf{p} \right) = \left\{ \symbf{x} \in \R^n : \norm{\symbf{x} - \symbf{p}} < r \right\}.
    \end{equation*}
    This region includes all points less than a distance \(r\) from the
    vector \(\symbf{p}\), where the distance is typically defined by the
    \(L_2\)-norm:
    \begin{equation*}
        \norm{\symbf{x} - \symbf{p}}_2 = \left( \sum_{i=1}^n \left( x_i - p_i \right)^2 \right)^{1/2}.
    \end{equation*}
\end{definition}
\begin{definition}[Open Set]
    A set \(S \subset \R^n\) is open if for every point \(\symbf{x} \in S\),
    there exists \(\delta > 0\), such that the open ball
    \(B_{\delta}\left( \symbf{x} \right) \in S\).
\end{definition}
\begin{definition}[Closed Set]
    A set \(S \subset \R^n\) is closed if its complement
    \(\R^n \setminus S\) is open.
\end{definition}
\begin{definition}[Connected Set]
    A set \(S \subset \R^n\) is connected if it cannot be represented as
    the union of two or more disjoint non-empty open subsets.
\end{definition}
\begin{definition}[Path Connected Set]
    A set \(S \subset \R^n\) is path connected if for every pair of
    points \(\symbf{x}, \symbf{y} \in S\), there exists a continuous
    path \(\symbf{r}\left( t \right)\) from \(\symbf{x}\) to \(\symbf{y}\).
\end{definition}
\begin{definition}[Simply Connected Set]
    A set \(S \subset \R^n\) is simply connected if it is path-connected
    and if every closed path in \(S\) can be continuously contracted to
    a point in \(S\).
\end{definition}
\subsection{Parametrisations of Curves}
\begin{definition}[Path]
    A path is a continuous function
    \begin{equation*}
        \symbf{r} : \interval{a}{b} \subset \R \to \mathscr{C} \subset \R^n
    \end{equation*}
    where \(t \mapsto \symbf{r}\left( \symbf{x}\left( t \right) \right)\)
    is the parameter of the path.
\end{definition}
\begin{definition}[Curve]
    A curve \(\mathscr{C}\) is the set of points in \(\R^n\)
    corresponding to the range of a path \(\symbf{r}\).
    \begin{equation*}
        \mathscr{C} = \left\{ \symbf{r}\left( t \right) : t \in \rinterval{a}{b} \right\}
    \end{equation*}
\end{definition}
A curve \(\mathscr{C}\) is \textbf{parametrised} by \(\symbf{r}\left( t \right)\),
and a path \(\symbf{r}\left( t \right)\) is a \textbf{parametrisation}
of \(\mathscr{C}\).
\begin{definition}[Closed Path]
    A path is closed if it starts and ends at the same point:
    \begin{equation*}
        \symbf{r}\left( a \right) = \symbf{r}\left( b \right).
    \end{equation*}
\end{definition}
\begin{definition}[Simple Path]
    A path is simple if the map \(t \mapsto \symbf{r}\left( t \right)\)
    is injective. That is, the path does not intersect itself.
\end{definition}
\begin{definition}[Regular Path]
    A path is regular (or smooth) if has nonzero continuous first derivatives:
    \begin{equation*}
        \symbf{r}\left( t \right) \in C^1 \quad \text{and} \quad
        \symbf{r}'\left( t \right) \neq \symbf{0}
    \end{equation*}
    for all \(t\). This restriction ensures that the path does not have
    any cusps and allows a unit tangent vector to be defined.
\end{definition}
\begin{definition}[Piecewise Regular Path]
    A path is piecewise regular if it can be divided into a finite
    number of regular paths.
\end{definition}
\begin{definition}[Path Concatenation]
    The path concatenation of two paths \(\symbf{r}_1 : \interval{a}{b} \subset \R \to \R^n\)
    and \(\symbf{r}_2 : \interval{b}{c} \subset \R \to \R^n\) is defined as
    \begin{equation*}
        \symbf{r}\left( t \right) = \left( \symbf{r}_1 \vee \symbf{r}_2 \right)\left( t \right) =
        \begin{cases}
            \symbf{r}_1\left( t \right) & t \in \interval{a}{b} \\
            \symbf{r}_2\left( t \right) & t \in \interval{b}{c}
        \end{cases}
    \end{equation*}
    where \(a < b < c\).
\end{definition}
\begin{definition}[Travelling Direction]
    The travelling direction of a path \(\symbf{r}\left( t \right)\) is
    the direction of increasing \(t\).
    A regular curve can be oriented by choosing one of the two
    travelling directions.
\end{definition}
\subsubsection{Remarks}
\begin{itemize}
    \item A curve is closed/simple/(piecewise) regular if it has a
          (closed/simple)/(piecewise) regular parametrisation.
    \item The implicit/explicit Cartesian representation of a curve is
          a curve, as it describes a set of points.
    \item The parametric representation of a curve is a path, as it
          includes the timing of the points.
    \item Converting from a curve to a path introduces a parameter.
    \item Converting from a path to a curve eliminates a parameter.
    \item \(\symbf{r}'\left( t \right)\) is the velocity vector of a path.
    \item \(\norm*{\symbf{r}'\left( t \right)}\) is the speed of a path.
    \item \(\symbf{r}''\left( t \right)\) is the acceleration vector of a path.
    \item Parametrisations are not unique.
\end{itemize}
\subsubsection{Reparametrisation}
Let \(\mathscr{C}\) be a curve parametrised by \(\symbf{r}\left( t
\right)\) with \(t \in \interval{a}{b}\). If there exists a bijective
map \(t = \theta\left( u \right)\), defined by \(\theta :
\interval{c}{d} \to \interval{a}{b}\) such that
\begin{equation*}
    \symbf{r}\left( \theta\left( u \right) \right) = \tilde{\symbf{r}}\left( u \right),
\end{equation*}
where
\begin{itemize}
    \item \(\theta\left( u \right)\) is continuously differentiable, and
    \item \(\theta'\left( u \right) \neq 0\) for all \(u \in \interval{c}{d}\),
\end{itemize}
then \(\tilde{\symbf{r}}\left( u \right)\) is a reparametrisation of
\(\symbf{r}\left( t \right)\) for \(u \in \interval{c}{d}\), and a
parametrisation of \(\mathscr{C}\). The map \(\theta\left( u \right)\)
guarantees that the simple/regular properties of \(\symbf{r}\left( t \right)\)
are preserved in \(\tilde{\symbf{r}}\left( u \right)\).
\begin{itemize}
    \item If \(\theta'\left( u \right) > 0\), then \(\symbf{r}\left( t
          \right)\) and \(\tilde{\symbf{r}}\left( u \right)\) are
          \textbf{equivalent} parametrisations.
    \item If \(\theta'\left( u \right) < 0\), then \(\symbf{r}\left( t
          \right)\) and \(\tilde{\symbf{r}}\left( u \right)\) are
          \textbf{opposite} parametrisations.
\end{itemize}
The unit tangent vectors of \(\symbf{r}\left( t \right)\) and
\(\tilde{\symbf{r}}\left( u \right)\) are related by the chain rule:
\begin{equation*}
    \tilde{\symbf{r}}'\left( u \right) = \symbf{r}'\left( \theta\left( u \right) \right) \theta'\left( u \right) \implies \norm*{\tilde{\symbf{r}}'\left( u \right)} = \norm*{\symbf{r}'\left( \theta\left( u \right) \right)} \abs*{\theta'\left( u \right)}
\end{equation*}
so that by dividing the first result by the second, we obtain
\begin{align*}
    \frac{\symbf{r}'\left( t \right)}{\norm*{\symbf{r}'\left( t \right)}} & = \frac{\tilde{\symbf{r}}'\left( u \right)}{\norm*{\tilde{\symbf{r}}'\left( u \right)}} \frac{\theta'\left( u \right)}{\abs*{\theta'\left( u \right)}} \\
                                                                          & = \sign{\left( \theta'\left( u \right) \right)} \frac{\tilde{\symbf{r}}'\left( u \right)}{\norm*{\tilde{\symbf{r}}'\left( u \right)}}                  \\
                                                                          & =
    \begin{cases}
        \displaystyle\frac{\tilde{\symbf{r}}'\left( u \right)}{\norm*{\tilde{\symbf{r}}'\left( u \right)}}  & \text{if } \theta'\left( u \right) > 0 \\[2.5ex]
        -\displaystyle\frac{\tilde{\symbf{r}}'\left( u \right)}{\norm*{\tilde{\symbf{r}}'\left( u \right)}} & \text{if } \theta'\left( u \right) < 0
    \end{cases}
\end{align*}
\subsection{Common Parametrisations of Curves}
To parametrise a curve, consider the following strategies:
\begin{itemize}
    \item For a closed curve, consider the polar parametrisation in
          terms of the angle \(\theta\):
          \begin{equation*}
              \symbf{r}\left( \theta \right) = \abracket*{R\left( \theta \right) \cos{\left( \theta \right)},\: R\left( \theta \right) \sin{\left( \theta \right)}}.
          \end{equation*}
    \item For a curve that is the intersection of two surfaces,
          consider one of the following mappings:
          \begin{equation*}
              x \mapsto
              \begin{bmatrix}
                  x                 \\
                  y\left( x \right) \\
                  z\left( x \right)
              \end{bmatrix}
              \qquad
              y \mapsto
              \begin{bmatrix}
                  x\left( y \right) \\
                  y                 \\
                  z\left( y \right)
              \end{bmatrix}
              \qquad
              z \mapsto
              \begin{bmatrix}
                  x\left( z \right) \\
                  y\left( z \right) \\
                  z
              \end{bmatrix}
          \end{equation*}
    \item Otherwise, consider a vector construction.
\end{itemize}
\subsubsection{Line Segments}
To parametrise a line segment from point \(A\) to \(B\), define the
parameter \(t \in \interval{0}{1}\). Then, consider the vectors
\(\symbf{a} = \overline{OA}\) and \(\symbf{b} = \overline{OB}\). By
scaling the vector from \(A\) to \(B\) by \(t\), we can parametrise the
line segment as
\begin{equation*}
    \symbf{r}\left( t \right) = \symbf{a} + t \left( \symbf{b} - \symbf{a} \right) = \symbf{a} \left( 1 - t \right) + \symbf{b} t.
\end{equation*}
\subsubsection{Circles}
To parametrise a circle of radius \(R\) centred at the
\(\abracket*{x_0,\: y_0}\), first parametrise the curve in terms of the
angle \(\theta\), then shift the curve by \(\abracket*{x_0,\: y_0}\).
\begin{equation*}
    \symbf{r}\left( \theta \right) =
    \begin{bmatrix}
        x_0 \\
        y_0
    \end{bmatrix}
    +
    \begin{bmatrix}
        R \cos{\left( \theta \right)} \\
        R \sin{\left( \theta \right)}
    \end{bmatrix}
    =
    \begin{bmatrix}
        x_0 + R \cos{\left( \theta \right)} \\
        y_0 + R \sin{\left( \theta \right)}
    \end{bmatrix}
\end{equation*}
\subsubsection{Velocity Vectors}
The velocity vector of a parametrised curve \(\symbf{r}\left( t \right)
=
\begin{bmatrix}
    x_1\left( t \right) \\
    x_2\left( t \right) \\
    \vdots              \\
    x_n\left( t \right)
\end{bmatrix}
\) is defined as
\begin{equation*}
    \symbf{v}\left( t \right) = \symbf{r}'\left( t \right) = \lim_{\Delta t \to 0} \frac{\symbf{r}\left( t + \Delta t \right) - \symbf{r}\left( t \right)}{\Delta t} =
    \begin{bmatrix}
        x_1'\left( t \right) \\
        x_2'\left( t \right) \\
        \vdots               \\
        x_n'\left( t \right)
    \end{bmatrix}
\end{equation*}
where \(\symbf{v}\left( t \right)\) is a tangent vector to the curve at the point \(\symbf{r}\left( t \right)\), for all \(t\).
\subsubsection{Tangent Vectors}
Following from the definition of the velocity vector, the tangent
vectors of a parametrised curve are unit vectors in the direction of
the velocity vector.
\begin{equation*}
    \hat{\symbf{\tau}}\left( t \right) = \pm \frac{\symbf{v}\left( t \right)}{\norm{\symbf{v}\left( t \right)}} = \pm \hat{\symbf{v}}\left( t \right)
\end{equation*}
For a curve given in explicit form \(y = f\left( x \right)\), the tangent vectors are given by
\begin{equation*}
    \hat{\symbf{\tau}}\left( x \right) = \pm \frac{1}{\sqrt{1 + \left( f'\left( x \right) \right)^2}}
    \begin{bmatrix}
        1 \\
        f'\left( x \right)
    \end{bmatrix}
\end{equation*}
\subsection{Parametrisations of Surfaces}
\begin{definition}[Homeomorphism]
    A homeomorphism is a bijective map between two topological spaces
    \(T\) and \(S\) that preserves the continuity of both the function
    and its inverse.
\end{definition}
\begin{definition}[Parametric Surface]
    A parametric surface \(S \subset \R^3\) is the set of points
    \linebreak corresponding to the range of the parametric function
    \begin{equation*}
        \symbf{r} : T \subset \R^2 \to S \subset \R^3
    \end{equation*}
    where \(T\) is open and connected
    \begin{equation*}
        S = \left\{ \symbf{r}\left( s,\: t \right) : \abracket*{s,\: t} \in T \right\}.
    \end{equation*}
    Here \(\symbf{r}\) is a homeomorphism between \(T\) and \(S\) of
    class \(C^1\), and the Jacobian matrix of \(\symbf{r}\) has rank 2
    for all \(\abracket*{s,\: t} \in T\).
    In other words, the vectors \(\symbf{r}_u = \pdv{\symbf{r}}{u}\) and
    \(\symbf{r}_v = \pdv{\symbf{r}}{v}\) are linearly independent so
    that
    \begin{equation*}
        \symbf{r}_u \times \symbf{r}_v \neq \symbf{0}.
    \end{equation*}
    This condition is necessary to ensure that the tangent plane and
    to the surface is well-defined, and allows for the definition of
    the unit normal vector
    \begin{equation*}
        \hat{\symbf{n}} = \frac{\symbf{r}_u \times \symbf{r}_v}{\norm{\symbf{r}_u \times \symbf{r}_v}}.
    \end{equation*}
\end{definition}
\begin{definition}[Chart]
    The inverse of a parametric surface \(\symbf{r}\) is called a chart:
    \begin{equation*}
        \symbf{r}^{-1} : S \subset \R^3 \to T \subset \R^2
    \end{equation*}
\end{definition}
\begin{definition}[Surface]
    \(\mathscr{S}\) is a surface if it is the union of parametric
    surfaces \(S_i\) such that the intersection of any two surfaces
    \(S_i \cap S_j\) is empty, a point, or a curve.
\end{definition}
\begin{definition}[Orientable Surface]
    A surface \(\mathscr{S}\) is orientable if there exists a
    continuous unit normal vector \(\hat{\symbf{n}}\) defined everywhere
    on \(\mathscr{S}\).
\end{definition}
\subsection{Common Parametrisations of Surfaces}
To parametrise an implicit surface \(z = f\left( x,\: y \right)\),
consider the parametrisation
\begin{equation*}
    \symbf{r}\left( x,\: y \right) = \abracket*{x,\: y,\: f\left( x,\: y \right)}.
\end{equation*}
\subsubsection{Planes}
To parametrise a plane that passes through the points \(A\), \(B\), and
\(C\), define a vector from the origin to \(C\), and two vectors from
\(C\) to \(A\) and \(B\):
\begin{equation*}
    \symbf{c} = \overline{OC},\: \symbf{a} = \overline{CA},\: \symbf{b} = \overline{CB}.
\end{equation*}
Then, consider the parametrisation
\begin{equation*}
    \symbf{r}\left( s,\: t \right) = \symbf{c} + s \symbf{a} + t \symbf{b}.
\end{equation*}
The unit normal vector of the plane is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_s \times \symbf{r}_t}{\norm{\symbf{r}_s \times \symbf{r}_t}} =
    \pm \frac{\symbf{a} \times \symbf{b}}{\norm{\symbf{a} \times \symbf{b}}}.
\end{equation*}
\subsubsection{Cylindrical Shells}
A cylindrical shell of radius \(R\) is parametrised by
\begin{equation*}
    \symbf{r}\left( \theta,\: z \right) =
    \begin{bmatrix}
        R \cos{\theta} \\
        R \sin{\theta} \\
        z
    \end{bmatrix}
\end{equation*}
for \(\theta \in \rinterval{0}{2 \pi}\) and \(z \in \R\).
The unit normal vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_\theta \times \symbf{r}_z}{\norm{\symbf{r}_\theta \times \symbf{r}_z}} =
    \pm
    \begin{bmatrix}
        \cos{\theta} \\
        \sin{\theta} \\
        0
    \end{bmatrix}
\end{equation*}
\subsubsection{Spherical Shells}
A spherical shell of radius \(R\) is parametrised by
\begin{equation*}
    \symbf{r}\left( \phi,\: \theta \right) =
    \begin{bmatrix}
        R \sin{\phi} \cos{\theta} \\
        R \sin{\phi} \sin{\theta} \\
        R \cos{\phi}
    \end{bmatrix}
\end{equation*}
for \(\phi \in \ointerval{0}{\pi}\) and \(\theta \in \ointerval{0}{2 \pi}\).
The unit normal vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_\phi \times \symbf{r}_\theta}{\norm{\symbf{r}_\phi \times \symbf{r}_\theta}} =
    \pm
    \begin{bmatrix}
        \sin{\phi} \cos{\theta} \\
        \sin{\phi} \sin{\theta} \\
        \cos{\phi}
    \end{bmatrix}
\end{equation*}
\subsubsection{Conical Shells}
A conical shell with opening angle \(\alpha\) is parametrised by
\begin{equation*}
    \symbf{r}\left( r,\: \theta \right) =
    \begin{bmatrix}
        r \cos{\theta} \\
        r \sin{\theta} \\
        r \cot{\alpha}
    \end{bmatrix}
\end{equation*}
for \(r \in \ointerval{0}{\infty}\) and \(\theta \in \rinterval{0}{2 \pi}\).
The unit normal vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_r \times \symbf{r}_\theta}{\norm{\symbf{r}_r \times \symbf{r}_\theta}} =
    \pm
    \begin{bmatrix}
        -\cos{\alpha} \cos{\theta} \\
        -\cos{\alpha} \sin{\theta} \\
        \sin{\alpha}
    \end{bmatrix}
\end{equation*}
\subsubsection{Explicit Surfaces}
A general surface \(z = f\left( x,\: y \right)\) can be parametrised by
\begin{equation*}
    \symbf{r}\left( x,\: y \right) =
    \begin{bmatrix}
        x \\
        y \\
        f\left( x,\: y \right)
    \end{bmatrix}
\end{equation*}
The unit normal vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_x \times \symbf{r}_y}{\norm{\symbf{r}_x \times \symbf{r}_y}} =
    \pm \frac{1}{\sqrt{f_x^2 + f_y^2 + 1}}
    \begin{bmatrix}
        -f_x \\
        -f_y \\
        1
    \end{bmatrix}
\end{equation*}
\subsection{Mathematical Representation of Curves and Surfaces}
\begin{definition}[Degree of Freedom]
    The degree of freedom of a curve is the difference between the
    number of variables and the number of equations.
\end{definition}
\subsubsection{Curves in Explicit Form (2D)}
A curve \(\mathscr{C} \in \R^2\) can be represented explicitly as
\begin{equation*}
    y = f\left( x \right)
\end{equation*}
When \(n > 2\), it is not possible to describe a curve explicitly.
The degree of freedom for a 2D explicit curve is 1.
\subsubsection{Curves in Implicit Form}
A curve \(\mathscr{C} \in \R^n\) can be represented implicitly as the
intersection of \(n - 1\) surfaces.
\begin{equation*}
    \left\{
    \begin{aligned}
        F_1\left( \symbf{x},\: z \right)       & = 0 \\
        F_2\left( \symbf{x},\: z \right)       & = 0 \\
        \vdots                                 &     \\
        F_{n - 1}\left( \symbf{x},\: z \right) & = 0
    \end{aligned}
    \right.
\end{equation*}
The degree of freedom is 1.
\subsubsection{Curves in Parametric Form}
A curve \(\mathscr{C} \in \R^n\) can be represented parametrically as
\begin{equation*}
    \symbf{r}\left( t \right) =
    \begin{bmatrix}
        x_1\left( t \right) \\
        x_2\left( t \right) \\
        \vdots              \\
        x_n\left( t \right)
    \end{bmatrix}
\end{equation*}
where \(t\) is the parameter. The degree of freedom is \(n - 1\).
\subsubsection{Surfaces in Explicit Form}
A surface \(\mathscr{S} \in \R^3\) can be represented explicitly as
\begin{equation*}
    z = f\left( x,\: y \right)
\end{equation*}
The degree of freedom is 2.
\subsubsection{Surfaces in Implicit Form}
A surface \(\mathscr{S} \in \R^3\) can be represented implicitly as
\begin{equation*}
    F\left( x,\: y,\: z \right) = 0
\end{equation*}
The degree of freedom is 2.
\subsubsection{Surfaces in Parametric Form}
A surface \(\mathscr{S} \in \R^3\) can be represented parametrically as
\begin{equation*}
    \symbf{r}\left( s,\: t \right) =
    \begin{bmatrix}
        x\left( s,\: t \right) \\
        y\left( s,\: t \right) \\
        z\left( s,\: t \right)
    \end{bmatrix}
\end{equation*}
where \(s\) and \(t\) are the parameters. The degree of freedom is 1.
\subsection{Converting Between Representations}
\subsubsection{Explicit to Implicit}
The equation \(z = f\left( \symbf{x} \right)\) can always be converted
to implicit form by rewriting it as
\begin{equation*}
    F\left( \symbf{x},\: z \right) = z - f\left( \symbf{x} \right) = 0.
\end{equation*}
\subsubsection{Implicit to Explicit}
The equation \(F\left( \symbf{x},\: z \right) = 0\) can be converted to
explicit form if we can solve for \(z\) in terms of \(\symbf{x}\).
\subsubsection{Parametric to Explicit/Implicit}
The equation \(\symbf{r}\left( \symbf{t} \right) = \abracket*{x_1\left(
\symbf{t} \right),\: \dots,\: x_n\left( \symbf{t} \right)}\) can be
written in explicit or implicit form, if the parameters \(\symbf{t}\)
can be eliminated from the simultaneous equations.
\subsubsection{Explicit to Parametric}
The equation \(z = f\left( \symbf{x} \right)\) can always be converted
to parametric form by choosing the parameter \(\symbf{t} = \symbf{x}\),
so that
\begin{equation*}
    \symbf{r}\left( \symbf{t} \right) = \abracket*{\symbf{t},\: f\left( \symbf{t} \right)}.
\end{equation*}
\subsubsection{Implicit to Parametric}
The equation \(F\left( \symbf{x} \right) = 0\) can be converted to
parametric form if we can find \(\symbf{x} = \symbf{r}\left( \symbf{t}
\right)\) such that \(F\left( \symbf{r}\left( \symbf{t} \right) \right)
= 0\), and
\begin{equation*}
    \symbf{r}\left( \symbf{t} \right) = \abracket*{x_1\left( \symbf{t} \right),\: \dots,\: x_n\left( \symbf{t} \right)}.
\end{equation*}
for all \(t\).
\section{Multivariable Calculus}
\subsection{Multivariable Functions}
\begin{definition}[Multivariable Function]
    A multivariable function \(f\) maps several independent variables to a real number:
    \begin{equation*}
        f : E \subset \R^n \to \R
    \end{equation*}
    where
    \begin{align*}
        \abracket*{x_1,\: x_2,\: \ldots,\: x_n} & \mapsto z = f\left( x_1,\: x_2,\: \ldots,\: x_n \right) \\
        \symbf{x}                               & \mapsto z = f\left( \symbf{x} \right)
    \end{align*}
\end{definition}
\begin{definition}[Domain]
    The domain of \(f\) is the subset of \(\R^n\) for which \(f\) is defined.
    It corresponds to the set of all possible inputs to \(f\).
    \begin{equation*}
        \mathscr{D} \left( f \right) = E
    \end{equation*}
\end{definition}
\begin{definition}[Range]
    The range of \(f\) is the image of the domain \(E\) under \(f\):
    \begin{equation*}
        \mathscr{R} \left( f \right) = \left\{ f\left( \symbf{x} \right) \in \R : \symbf{x} \in E \right\}
    \end{equation*}
\end{definition}
\begin{definition}[Graph]
    The graph of \(f\) is defined as the set
    \begin{equation*}
        G = \left\{ \abracket*{\symbf{x},\: f\left( \symbf{x} \right)} : \symbf{x} \in E \right\} \subset \R^{n + 1}
    \end{equation*}
\end{definition}
\subsection{Curves of Intersection}
\begin{definition}[Curves of Intersection]
    The curves of intersection of \(f\) with the plane \(x_i = c\) are defined as
    \begin{equation*}
        \left\{ \abracket*{x_1,\: x_2,\: \ldots,\: x_{i - 1},\: c,\: x_{i + 1},\: \ldots,\: x_n} : \abracket*{x_1,\: x_2,\: \ldots,\: x_n} \in E \right\}
    \end{equation*}
    In 3D, curves of intersection of \(z = f\left( x,\: y \right)\) with the planes perpendicular to the
    \(x\), \(y\), or \(z\)-axes allow us to represent the function in 3D.
    \begin{align*}
        \perp x \: (x = c)                             &  & \perp y \: (y = c)                             &  & \perp z \: (z = c)                                        \\
        z = f\left( c,\: y \right) = g\left( y \right) &  & z = f\left( x,\: c \right) = g\left( x \right) &  & c = f\left( x,\: y \right) \implies y = g\left( x \right)
    \end{align*}
\end{definition}
\begin{definition}[Level Set]
    The level sets of \(f\) are the set of all points in the domain of \(f\) that map to a given value \(c\).
    \begin{equation*}
        \left\{ \symbf{x} \in E : f\left( \symbf{x} \right) = c \right\}
    \end{equation*}
    In 2D, level sets are called \textbf{level curves}.
\end{definition}
\begin{definition}[Contour Map]
    The projection of all level curves onto the \(xy\)-plane is called the \textbf{contour map} of \(f\).
    The lines of a contour map are called \textbf{contours}.
\end{definition}
\begin{theorem}[Level Sets and Gradients]
    The level set of a function \(f\) is perpendicular to the gradient
    of \(f\).
\end{theorem}
\subsection{Derivatives}
\begin{definition}[Continuity]
    A function \(f\) is continuous at a point \(\symbf{x}_0\) if
    \begin{equation*}
        \forall \varepsilon > 0,\: \exists \delta > 0 : \norm{\symbf{x} - \symbf{x}_0} < \delta \implies \norm{f\left( \symbf{x} \right) - f\left( \symbf{x}_0 \right)} < \varepsilon
    \end{equation*}
    Equivalently, \(f\) is continuous at \(\symbf{x}_0\) when
    \begin{equation*}
        \lim_{n \to \infty} f\left( \symbf{x}_n \right) = f\left( \symbf{x}_0 \right)
    \end{equation*}
    for all sequences \(\left\{ \symbf{x}_n \right\} \) where \(\symbf{x}_n \to \symbf{x}_0\).

    A function \(f\) is continuous on a set \(E\) if it is continuous
    at every point in \(E\).
\end{definition}
\begin{definition}[Partial Derivatives]
    Partial derivatives represent the rate of change of a function with respect to one of its variables, holding all other variables constant.
    \begin{equation*}
        \pdv{f}{x_i} = f_{x_i} = \lim_{h \to 0} \frac{f\left( x_1,\: x_2,\: \ldots,\: x_i + h,\: \ldots,\: x_n \right) - f\left( x_1,\: x_2,\: \ldots,\: x_i,\: \ldots,\: x_n \right)}{h}
    \end{equation*}
\end{definition}
\begin{definition}[Higher-order Partial Derivatives]
    Higher-order partial derivatives are defined as
    \begin{equation*}
        \pdv[order=n]{f}{x_i} = \pdv{}{x_i} \left( \pdv[order={n - 1}]{f}{x_i} \right)
    \end{equation*}
\end{definition}
\begin{definition}[Mixed Partial Derivatives]
    Mixed partial derivatives are given the following notation
    \begin{equation*}
        \pdv{f}{x_i,x_j} = \pdv{}{x_i} \left( \pdv{f}{x_j} \right) = f_{x_i x_j}
    \end{equation*}
\end{definition}
\begin{theorem}[Schwarz's Theorem (or Clairaut's Theorem)]
    For a function \(f : E \subset \R^2 \to \R\) with \(\symbf{x}_a \in E\), if \(f_{xy}\) and \(f_{yx}\) are continuous on \(\symbf{x}_0\), then
    \begin{equation*}
        f_{xy} = f_{yx}
    \end{equation*}
\end{theorem}
\begin{remark}
    For an arbitrary \(n\) and \(k \leqslant n\), if all partials of order \(\leqslant \! k\) are continuous in the neighbourhood of \(\symbf{x}_0\),
    then mixed partials of order \(k\) are equal for any permutation of indices:
    \begin{equation*}
        \pdv[mixed-order=k]{f}{x_{i_1}\ldots,x_{i_k}} = \pdv[mixed-order=k]{f}{x_{j_1}\ldots,x_{j_k}}
    \end{equation*}
\end{remark}
\subsection{Chain Rule}
For a function with multiple arguments, each argument of \(f\) that has
an implicit dependence on the variable of differentiation must be
differentiated using the chain rule, where all contributions are
summed.
\begin{gather*}
    \odv{}{t} f\left( x\left( t \right),\: y\left( t \right) \right) = \pdv{f}{x} \odv{x}{t} + \pdv{f}{y} \odv{y}{t} \\
    \pdv{}{u} f\left( x\left( u,\: v \right),\: y\left( u,\: v \right) \right) = \pdv{f}{x} \pdv{x}{u} + \pdv{f}{y} \pdv{y}{u}
\end{gather*}
\begin{definition}[Total Derivative]
    The total derivative of a function \(f : \R^n \to \R\) is defined as
    \begin{equation*}
        \odv{f}{t} = \sum_{i = 1}^n \pdv{f}{x_i} \odv{x_i}{t} = \pdv{f}{x_1} \odv{x_1}{t} + \pdv{f}{x_2} \odv{x_2}{t} + \cdots + \pdv{f}{x_n} \odv{x_n}{t}
    \end{equation*}
\end{definition}
\begin{proof}
    Consider the case where \(n = 2\). The total derivative can be written as
    \begin{equation*}
        \odv{}{t} f\left( x\left( t \right),\: y\left( t \right) \right) = \lim_{\Delta{t} \to 0} \frac{1}{\Delta{t}} \left[ f\left( x\left( t + \Delta{t} \right),\: y\left( t + \Delta{t} \right) \right) - f\left( x\left( t \right),\: y\left( t \right) \right) \right]
    \end{equation*}
    Using the substitutions \(x\left( t + \Delta{t} \right) = x\left( t \right) + \Delta{x}\) and \(y\left( t + \Delta{t} \right) = y\left( t \right) + \Delta{y}\),
    \begin{align*}
        \odv{f}{t} & = \lim_{\Delta{t} \to 0} \frac{1}{\Delta{t}} \left[ f\left( x + \Delta{x},\: y + \Delta{y} \right) - f\left( x,\: y \right) \right]                                                                                                                                                   \\
                   & = \lim_{\Delta{t} \to 0} \frac{1}{\Delta{t}} \left[ f\left( x + \Delta{x},\: y + \Delta{y} \right) - f\left( x,\: y + \Delta{y} \right) + f\left( x,\: y + \Delta{y} \right) - f\left( x,\: y \right) \right]                                                                         \\
                   & = \lim_{\Delta{t} \to 0} \left[ \frac{\Delta{x}}{\Delta{t}} \frac{f\left( x + \Delta{x},\: y + \Delta{y} \right) - f\left( x,\: y + \Delta{y} \right)}{\Delta{x}} + \frac{\Delta{y}}{\Delta{t}} \frac{f\left( x,\: y + \Delta{y} \right) - f\left( x,\: y \right)}{\Delta{y}} \right] \\
                   & = \odv{x}{t} \pdv{f}{x} + \odv{y}{t} \pdv{f}{y}
    \end{align*}
\end{proof}
\begin{definition}[Gradient]
    The gradient is an operator that collects all partial derivatives of a function into a vector.
    \begin{equation*}
        \symbf{\nabla} =
        \begin{bmatrix}
            \partial_{x_1} \\
            \partial_{x_2} \\ \vdots \\
            \partial_{x_n}
        \end{bmatrix}
        \implies \symbf{\nabla} f =
        \begin{bmatrix}
            \partial_{x_1} f \\
            \partial_{x_2} f \\ \vdots \\
            \partial_{x_n} f
        \end{bmatrix}
    \end{equation*}
\end{definition}
The gradient allows us to define the chain rule using the dot product:
\begin{equation*}
    \odv{f}{t} = \symbf{\nabla} f \cdot \odv{\symbf{x}}{t}
\end{equation*}
\subsection{Directional Derivatives}
The directional derivative of a function \(f : E \subset \R^n \to \R\)
at a point \(\symbf{x}_0 \in E\) in the direction of a unit vector
\(\hat{\symbf{u}} \in \R^n\) is the slope of \(f\) in the direction of
\(\symbf{u}\):
\begin{equation*}
    D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \partial_{\symbf{u}} f\left( \symbf{x}_0 \right) = \lim_{h \to 0}
     \frac{f\left( \symbf{x}_0 + h \hat{\symbf{u}} \right) - f\left(
     \symbf{x}_0 \right)}{h}
\end{equation*}
\begin{proposition}
    The directional derivative can be computed using the gradient of \(f\)
    \begin{equation*}
        D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \symbf{\nabla} f\left( \symbf{x}_0 \right) \cdot \hat{\symbf{u}}.
    \end{equation*}
\end{proposition}
\begin{proof}
    Consider the path in the output space \(g\left( s \right) = f\left( \symbf{x}_0 + s \hat{\symbf{u}} \right) \in \R\) for \(s \in \R\).
    The derivative of \(g\) w.r.t.\ \(s\) is given by:
    \begin{equation*}
        \odv{g}{s} = \lim_{h \to 0} \frac{g\left( s + h \right) - g\left( s \right)}{h}
    \end{equation*}
    where if we evaluate \(g\) at \(s = 0\), we get
    \begin{equation*}
        \odv{g}{s}_{s = 0} = \lim_{h \to 0} \frac{g\left( h \right) - g\left( 0 \right)}{h} = \lim_{h \to 0} \frac{f\left( \symbf{x}_0 + h \hat{\symbf{u}} \right) - f\left( \symbf{x}_0 \right)}{h} = D_{\symbf{u}} f\left( \symbf{x}_0 \right).
    \end{equation*}
    Using the derivative of the parametrised vector \(\symbf{x} = \symbf{x}_0 + s \hat{\symbf{u}} \in \R^n\):
    \begin{equation*}
        \odv{\symbf{x}}{s} = \hat{\symbf{u}}
    \end{equation*}
    we can evaluate the derivative of \(g\) w.r.t.\ \(s\) using the total derivative:
    \begin{align*}
        \odv{g}{s}_{s = 0} & = \sum_{i = 1}^n \pdv{f}{x_i}_{s = 0} \odv{x_i}{s}_{s = 0}               \\
                           & = \sum_{i = 1}^n \pdv{f}{x_i}_{s = 0} \hat{u}_i                          \\
                           & = \symbf{\nabla} f\left( \symbf{x} \right)_{s = 0} \cdot \hat{\symbf{u}} \\
                           & = \symbf{\nabla} f\left( \symbf{x}_0 \right) \cdot \hat{\symbf{u}}
    \end{align*}
    where \(\hat{u}_i\) is the \(i\)-th component of \(\hat{\symbf{u}}\).
    Therefore \(D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \symbf{\nabla} f\left( \symbf{x}_0 \right) \cdot \hat{\symbf{u}}\).
\end{proof}
\begin{remark}
    Partial derivatives are directional derivatives in the direction of the canonical basis vectors \(\hat{\symbf{e}}_i\).
    \begin{equation*}
        \pdv{f}{x_i} = D_{\hat{\symbf{e}}_i} f
    \end{equation*}
    We can therefore say that the directional derivative is a generalisation of the partial derivative
    for any direction \(\hat{\symbf{u}}\).
\end{remark}
\begin{proposition}
    The gradient of a function \(\symbf{\nabla} f\) is orthogonal to the level curves of \(f\).
\end{proposition}
\begin{proof}
    Consider the path \(\symbf{x}\left( s \right)\) on a contour of \(f\). As \(f\) is constant on the contour,
    \begin{equation*}
        \pdv{f}{s} = 0.
    \end{equation*}
    Using the chain rule,
    \begin{equation*}
        \pdv{f}{s} = \sum_{i = 1}^n \pdv{f}{x_i} \pdv{x_i}{s} = \symbf{\nabla} f \cdot \odv{\symbf{x}}{s} = 0
    \end{equation*}
    therefore as the dot product is zero, \(\symbf{\nabla} f\) is orthogonal to the path \(\symbf{x}\left( s \right)\).
\end{proof}
\begin{proposition}
    The directional derivative is maximised when \(\hat{\symbf{u}}\) is parallel to \(\symbf{\nabla} f\).
\end{proposition}
\begin{proof}
    Using the angle definition of the dot product, the directional derivative is given by
    \begin{equation*}
        D_{\symbf{u}} f = \symbf{\nabla} f \cdot \hat{\symbf{u}} = \norm{\symbf{\nabla} f} \norm{\hat{\symbf{u}}} \cos{\theta} = \norm{\symbf{\nabla} f} \cos{\theta}
    \end{equation*}
    This expression is maximised when \(\cos{\theta} = 1\), or when \(\symbf{u}\) is parallel to \(\symbf{\nabla} f\).
\end{proof}
\begin{remark}
    The maximum slope of \(f\) at \(\symbf{x}_0\) is given by the magnitude of the gradient at \(\symbf{x}_0\):
    \begin{equation*}
        \max_{\hat{\symbf{u}}} D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \norm{\symbf{\nabla} f\left( \symbf{x}_0 \right)}
    \end{equation*}
\end{remark}
\subsection{Normal Vectors to Curves}
\subsubsection{Parametric Curves}
For a parametric curve \(\symbf{r}\left( t \right)\), we find a normal
vector \(\hat{\symbf{n}}\) such that
\begin{equation*}
    \hat{\symbf{n}} \cdot \symbf{r}'\left( t \right) = 0
\end{equation*}
where \(\symbf{r}'\left( t \right)\) is a tangent vector to the curve.
\subsubsection{Implicit Curves}
For an implicit curve \(F\left( x,\: y \right) = 0\), the normal vector
is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{\nabla} F}{\norm{\symbf{\nabla} F}}
\end{equation*}
\subsubsection{Explicit Curves}
For an explicit curve \(y = f\left( x \right)\), we must convert the
curve to implicit or parametric form.
\subsection{Normal Vectors to Surfaces}
\subsubsection{Parametric Surfaces}
For a parametric surface \(\symbf{r}\left( s,\: t \right)\), the normal
vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_s \times \symbf{r}_t}{\norm{\symbf{r}_s \times \symbf{r}_t}}
\end{equation*}
where \(\symbf{r}_s\) and \(\symbf{r}_t\) are tangent vectors to the
surface.
\subsubsection{Implicit Surfaces}
For an implicit surface \(F\left( x,\: y,\: z \right) = 0\), the normal
vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{\nabla} F}{\norm{\symbf{\nabla} F}}
\end{equation*}
\subsubsection{Explicit Surfaces}
For an explicit surface \(z = f\left( x,\: y \right)\), we can either
convert the surface to implicit form, or consider the tangents of two
curves \(\symbf{r}_1\left( t \right)\) and \(\symbf{r}_2\left( t
\right)\) on the surface. Then we can find the normal vector by taking
the cross product of the tangent vectors:
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_1'\left( t \right) \times \symbf{r}_2'\left( t \right)}{\norm{\symbf{r}_1'\left( t \right) \times \symbf{r}_2'\left( t \right)}}
\end{equation*}
\subsection{Tangent Vectors to Curves}
\subsubsection{Parametric Curves}
For a parametric curve \(\symbf{r}\left( t \right)\), the tangent
vector is given by
\begin{equation*}
    \hat{\symbf{\tau}} = \pm \frac{\symbf{r}'\left( t \right)}{\norm{\symbf{r}'\left( t \right)}}
\end{equation*}
\subsubsection{Implicit Curves in 2D}
For an implicit curve \(F\left( x,\: y \right) = 0\), the tangent
vector can be found by first determining the normal vector
\(\hat{\symbf{n}}\) which is proportional to the gradient of \(F\):
\begin{equation*}
    \hat{\symbf{n}} =
    \begin{bmatrix}
        n_1 \\
        n_2
    \end{bmatrix}
    \propto \symbf{\nabla} f
\end{equation*}
such that the tangent vector is given by
\begin{equation*}
    \hat{\symbf{\tau}} = \pm
    \begin{bmatrix}
        -n_2 \\
        n_1
    \end{bmatrix}
\end{equation*}
\subsubsection{Implicit Curves in 3D}
Given the intersection of two implicit curves \(F\left( x,\: y,\: z
\right) = 0\) and \(G\left( x,\: y,\: z \right) = 0\), the tangent
vector along the intersection is given by
\begin{equation*}
    \hat{\symbf{\tau}} = \pm \frac{\symbf{\nabla} F \times \symbf{\nabla} G}{\norm{\symbf{\nabla} F \times \symbf{\nabla} G}}
\end{equation*}
\subsubsection{Explicit Curves in 2D}
For an explicit curve \(y = f\left( x \right)\), we can either:
\begin{itemize}
    \item convert the curve to parametric form: \(\symbf{r}\left( t
          \right) = \abracket*{t,\: f\left( t \right)}\)
    \item convert the curve to implicit form: \(F\left( x,\: y \right)
          = y - f\left( x \right) = 0\)
\end{itemize}
\subsection{Differentiability}
\subsection{Single Variable Functions}
\(f\left( x \right)\) is differentiable at \(a\) if \(f'\left( a \right) = \lim_{x \to a} \frac{f\left( x \right) - f\left( a \right)}{x - a}\) exists.
This is equivalent to saying that the tangent line at \(\abracket*{a,\: f\left( a \right)}\) is well-defined:
\begin{equation*}
    f\left( x \right) = \underbrace{f\left( a \right) + f'\left( a \right) f\left( x - a \right)}_{\text{tangent line}} + R\left( x - a \right) \qquad \text{with } \lim_{x \to a} \frac{R\left( x - a \right)}{x - a} = 0
\end{equation*}
\subsection{Multivariable Functions}
\(f\left( \symbf{x} \right)\) is differentiable at \(\symbf{a} \in \Omega \subset \R^n\) if there exists a linear map \(f'\left( \symbf{a} \right) :
\begin{cases*}
    \R^n \to \R                                      \\
    \xi \mapsto f'\left( \symbf{a} \right) \cdot \xi \\
\end{cases*}
\) such that
\begin{equation*}
    f\left( \symbf{x} \right) = \underbrace{f\left( \symbf{a} \right) + f'\left( \symbf{a} \right) \cdot \left( \symbf{x} - \symbf{a} \right)}_{\text{tangent plane}} + R\left( \symbf{x} - \symbf{a} \right) \qquad \text{with } \lim_{\symbf{x} \to \symbf{a}} \frac{R\left( \symbf{x} - \symbf{a} \right)}{\norm{\symbf{x} - \symbf{a}}} = 0
\end{equation*}
The linear map \(f'\left( \symbf{a} \right)\) is then the derivative of \(f\) at \(\symbf{a}\).
\begin{theorem}[Derivative Equivalence with Gradient]
    If \(f\) is differentiable, then
    \begin{equation*}
        f'\left( \symbf{a} \right) = \symbf{\nabla} f\left( \symbf{a} \right)
    \end{equation*}
    Additionally, if \(\symbf{\nabla} f\left( \symbf{a} \right)\) exists
    \(\forall \symbf{a} \in \Omega\), and all partial derivatives are
    continuous (i.e., \(\pdv{f}{x_i} : \Omega \subset \R^n \to R\) are
    continuous), then \(f\) is differentiable everywhere in \(\Omega\).
\end{theorem}
The first result tells us that the derivative is unique and that \(f\) is
differentiable at \(\symbf{a}\) if it has a tangent plane at \(\symbf{a}\).
The second result gives us a sufficient condition for differentiability.
\subsection{Taylor Series Expansion}
The Taylor series expansion of a function \(f\) at a point
\(\symbf{a}\) is given by
\begin{multline*}
    f\left( \symbf{x} \right) = f\left( \symbf{a} \right) + \sum_{i = 0}^n \pdv{f\left( \symbf{a} \right)}{x_i} \left( x_i - a_i \right) + \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \pdv{f\left( \symbf{a} \right)}{x_i,x_j} \left( x_i - a_i \right) \left( x_j - a_j \right) + \cdots \\
    + \frac{1}{k!} \sum_{i_1, \ldots, i_k}^{n} \pdv{f\left( \symbf{a} \right)}{x_{i_1}, \ldots, x_{i_k}} \left( x_{i_1} - a_{i_1} \right) \cdots \left( x_{i_k} - a_{i_k} \right) + \cdots
\end{multline*}
This allows us to compute the tangent plane, paraboloid, and so on, of
a function \(f\) at \(\symbf{a}\), by increasing the order \(k\) of the
Taylor series expansion.
\section{Double Integrals}
Integrals represent continuous sums of infinitesimal quantities, and
allow us to measure extensive and average properties of objects, such
as lengths, surface areas, volumes, masses, centres of mass, and so on.
\subsection{Riemann Sums}
The signed area under the function \(f\left( x \right)\) is represented
by
\begin{equation*}
    \int_a^b f\left( x \right) \odif{x} = \lim_{n \to \infty} \sum_{i = 1}^n f\left( x_i \right) \Delta x
\end{equation*}
Similarly, the signed volume under the surface \(f\left( x,\: y \right)\)
is represented by
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} = \lim_{\Delta A_i \to 0} \sum_{i = 1}^n f\left( x_i,\: y_i \right) \Delta A_i
\end{equation*}
where \(R\) is the region of integration.
\subsection{Lebesgue Integrals}
Lebesgue integrals are a top down approach to multiple integrals that
allow us to integrate a wider class of nonnegative functions. This
includes improper integrals where \(f\) is discontinuous or singular,
or when the domain of integration is unbounded.
\begin{theorem}[Fubini's Theorem]
    For a nonnegative function \(f : \R^2 \to \rinterval{0}{\infty}\),
    the following equality holds,
    \begin{equation*}
        0 \leqslant \iint_{\R^2} f\left( x,\: y \right) \odif{A} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{x} \right] \odif{y} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{y} \right] \odif{x} \leqslant \infty
    \end{equation*}
\end{theorem}
The above theorem holds for particular classes of functions. The
following techniques can be used to integrate arbitrary functions.
\subsubsection{Finite Domains}
For double integrals over domains \(R \subset \R^2\), we can define a
new function \(\tilde{f}\) such that
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} \equiv \iint_{\R^2} \tilde{f}\left( x,\: y \right) \odif{A}, \qquad \text{where } \tilde{f}\left( x,\: y \right) =
    \begin{cases}
        f\left( x,\: y \right) & \left( x,\: y \right) \in R \\
        0                      & \text{otherwise}
    \end{cases}
\end{equation*}
\subsubsection{Nonpositive Functions}
For nonpositive functions \(f : \R^2 \to \ointerval{-\infty}{\infty}\),
we can express \(f\) as \(f = f^{+} - f^{-}\), where \(f^{+}\) and
\(f^{-}\) are the positive and negative parts of \(f\) respectively:
\begin{align*}
    f^{+}\left( x,\: y \right) & = \max{\left\{ 0,\: f\left( x,\: y \right) \right\}}
    =
    \begin{cases}
        f\left( x,\: y \right) & f\left( x,\: y \right) \geqslant 0 \\
        0                      & f\left( x,\: y \right) < 0
    \end{cases}
    \\
    f^{-}\left( x,\: y \right) & = -\min{\left\{ 0,\: f\left( x,\: y \right) \right\}}
    =
    \begin{cases}
        -f\left( x,\: y \right) & f\left( x,\: y \right) < 0         \\
        0                       & f\left( x,\: y \right) \geqslant 0
    \end{cases}
\end{align*}
Therefore, the double integral is defined
\begin{equation*}
    \iint_{\R^2} f\left( x,\: y \right) \odif{A} = \iint_{\R^2} f^{+}\left( x,\: y \right) \odif{A} - \iint_{\R^2} f^{-}\left( x,\: y \right) \odif{A}
\end{equation*}
\begin{definition}[Integrable Function]
    A function \(f : \R^n \to \R\) is \textbf{integrable} if
    \begin{equation*}
        \int_{\R^n} \abs{f\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty
    \end{equation*}
\end{definition}
\begin{theorem}[Fubini's Theorem for Integrable Functions]
    If \(f : \R^2 \to \R\) is an integrable function
    \begin{equation*}
        \iint_{\R^2} f\left( x,\: y \right) \odif{A} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{x} \right] \odif{y} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{y} \right] \odif{x}
    \end{equation*}
\end{theorem}
\subsection{Measures}
\begin{definition}[Indicator Function]
    The indicator function of \(R\) is defined:
    \begin{equation*}
        1_R\left( x,\: y \right) =
        \begin{cases}
            1 & \left( x,\: y \right) \in R \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{definition}[Measure]
    The measure of a region \(R\) is defined as
    \begin{equation*}
        \mu\left( R \right) = \iint_{\R^2} 1_R \odif{A}
    \end{equation*}
    In two dimensions, the measure is the area of the region.
\end{definition}
\subsubsection{Rectangular Regions}
For a rectangular region \(R = \left[ a,\: b \right] \times \left[ c,\:
d \right]\), the indicator function is given by
\begin{align*}
    1_R\left( x,\: y \right) & =
    \begin{cases}
        1 & \left( x,\: y \right) \in D \\
        0 & \text{otherwise}
    \end{cases}
    \\
                             & = 1_{\left[ a,\: b \right]} \left( x \right) 1_{\left[ c,\: d \right]} \left( y \right)
\end{align*}
so that the area of \(R\) is given by
\begin{align*}
    \mu\left( R \right) & = \iint_{\R^2} 1_R \odif{A} = \int_{-\infty}^\infty \int_{-\infty}^\infty 1_{\left[ a,\: b \right]} \left( x \right) 1_{\left[ c,\: d \right]} \left( y \right) \odif{y} \odif{x} = \int_{-\infty}^\infty 1_{\left[ a,\: b \right]} \left( x \right) \int_{-\infty}^\infty 1_{\left[ c,\: d \right]} \left( y \right) \odif{y} \odif{x} \\
                        & = \int_a^b 1 \left[ \int_c^d 1 \odif{y} \right] \odif{x} = \int_a^b \left( d - c \right) \odif{x} = \left( b - a \right) \left( d - c \right)
\end{align*}
\subsubsection{Integrating Functions over Regions}
When integrating over a nonuniform density \(f\), we can multiply \(f\)
with the indicator function over \(R\):
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} = \iint_{\R^2} f\left( x,\: y \right) 1_R \odif{A}
\end{equation*}
\subsection{Simple Domains}
\subsubsection{Type I --- \texorpdfstring{\(y\)}{y}-Simple}
Let \(Y\) be defined as a region bounded by
\(x_1 \leqslant x \leqslant x_2\) and
\(y_1\left( x \right) \leqslant y \leqslant y_2\left( x \right)\).
\(Y\) is called \(y\)-simple as it can be split into subdomains \(R_i\)
with continuous lines \textbf{parallel to the \(y\)-axis}.

The double integral over \(Y\) can then be decomposed into the sum of
integrals over each subdomain \(Y_i\):
\begin{equation*}
    \iint_Y f\left( x,\: y \right) \odif{A} = \sum_{i = 1}^n \iint_{Y_i} f\left( x,\: y \right) \odif{A}
\end{equation*}
which is equivalent to integrating between the \(y\)-bounds of each
subdomain \(Y_i\) within the \(x\)-bounds of \(Y\):
\begin{equation*}
    \iint_Y f\left( x,\: y \right) \odif{A} = \int_{x_1}^{x_2} \left[ \int_{y_1\left( x \right)}^{y_2\left( x \right)} f\left( x,\: y \right) \odif{y} \right] \odif{x}
\end{equation*}
\subsubsection{Type II --- \texorpdfstring{\(x\)}{x}-Simple}
Let \(X\) be defined as a region bounded by
\(x_1\left( y \right) \leqslant x \leqslant x_2\left( y \right)\) and
\(y_1 \leqslant y \leqslant y_2\).
\(X\) is called \(x\)-simple as it can be split into subdomains \(R_i\)
with continuous lines \textbf{parallel to the \(x\)-axis}.

The double integral over \(X\) can then be decomposed into the sum of
integrals over each subdomain \(X_i\):
\begin{equation*}
    \iint_X f\left( x,\: y \right) \odif{A} = \sum_{i = 1}^n \iint_{X_i} f\left( x,\: y \right) \odif{A}
\end{equation*}
which is equivalent to integrating between the \(x\)-bounds of each
subdomain \(X_i\) within the \(y\)-bounds of \(X\):
\begin{equation*}
    \iint_X f\left( x,\: y \right) \odif{A} = \int_{y_1}^{y_2} \left[ \int_{x_1\left( y \right)}^{x_2\left( y \right)} f\left( x,\: y \right) \odif{x} \right] \odif{y}
\end{equation*}
\subsection{Transformation of Coordinates}
In single variable calculus, we can use the transformation \(x =
g\left( u \right)\) to change the variable of integration from \(x\) to
\(u\), using the chain rule, \(\odif{x} = g'\left( u \right)
\odif{u}\). For multiple integrals, the same can be accomplished using
the Jacobian matrix.
\begin{definition}[Jacobian Matrix]
    Consider the transformation \(\symbf{x} = T\left( \symbf{u} \right)\),
    where \(T : \R^m \to \R^n\) is once differentiable. The Jacobian
    matrix of \(T\) is defined as an \(m \times n\) matrix, denoted
    \(\symbf{J}\), whose \(\left( i,\: j \right)\)-th entry is given by
    \begin{equation*}
        \symbf{J}_{i,\: j} = \pdv{x_i}{u_j}.
    \end{equation*}
    Explicitly, the Jacobian matrix is given by
    \begin{equation*}
        \symbf{J} =
        \displaystyle
        \begin{bmatrix}
            \displaystyle\pdv{x_1}{u_1} & \displaystyle\cdots & \displaystyle\pdv{x_1}{u_n} \\
            \displaystyle\vdots         & \displaystyle\ddots & \displaystyle\vdots         \\
            \displaystyle\pdv{x_m}{u_1} & \displaystyle\cdots & \displaystyle\pdv{x_m}{u_n} \\
        \end{bmatrix}
        .
    \end{equation*}
    It may also be notated as
    \begin{equation*}
        \symbf{J} = \frac{\partial\left( x_1,\: \ldots,\: x_m \right)}{\partial\left( u_1,\: \ldots,\: u_n \right)}
    \end{equation*}
\end{definition}
\begin{definition}[Jacobian]
    When \(m = n\), we can determine the ratio of the area of the
    original region \(R\) to the area of the transformed region
    \(T\left( R \right)\) using the determinant of the Jacobian matrix:
    \begin{equation*}
        \abs*{\symbf{J}} = \det{\left( \symbf{J} \right)} = \abs*{\frac{\partial\left( x_1,\: \ldots,\: x_n \right)}{\partial\left( u_1,\: \ldots,\: u_n \right)}}.
    \end{equation*}
\end{definition}
In the case of two variables, \(\left( x,\: y \right) = T\left( u,\: v \right)\),
and the Jacobian matrix is defined as
\begin{equation*}
    \symbf{J} =
    \begin{bmatrix}
        \displaystyle\pdv{x}{u} & \displaystyle\pdv{x}{v} \\
        \displaystyle\pdv{y}{u} & \displaystyle\pdv{y}{v} \\
    \end{bmatrix}
    .
\end{equation*}
The Jacobian is then given by
\begin{equation*}
    \abs*{\symbf{J}} = \pdv{x}{u} \pdv{y}{v} - \pdv{x}{v} \pdv{y}{u}
\end{equation*}
\subsubsection{General Transformation}
Using this matrix determinant allows us to change the variables of
integration from \(\left( x,\: y \right)\) to \(\left( u,\: v \right)\)
using the following formula:
\begin{equation*}
    \iint_{R} f\left( x,\: y \right) \odif{x}\odif{y} = \iint_{T\left( R \right)} f\left( T\left( u,\: v \right) \right) \abs*{\det{\left( \symbf{J} \right)}} \odif{u}\odif{v}
\end{equation*}
where \(f\left( T\left( u,\: v \right) \right)\) is multiplied by the absolute value of the
Jacobian.
\subsubsection{Polar Coordinate Transformation}
\begin{definition}[Polar Coordinates]
    A polar coordinate system is defined by the transformation
    \begin{align*}
        x & = r \cos{\theta} \\
        y & = r \sin{\theta}
    \end{align*}
    where \(r > 0\) is the radius of the circle and \(\theta \in \rinterval{0}{2\pi}\)
    is the angle of rotation measured anticlockwise from the positive
    \(x\)-axis.
    The Jacobian is given by
    \begin{align*}
        \symbf{J}                      & =
        \begin{bmatrix}
            \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\theta} \\
            \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\theta} \\
        \end{bmatrix}
        \\
                                       & =
        \begin{bmatrix}
            \cos{\theta} & -r \sin{\theta} \\
            \sin{\theta} & r \cos{\theta}  \\
        \end{bmatrix}
        \\
        \det{\left( \symbf{J} \right)} & = r \cos^2{\theta} + r \sin^2{\theta} \\
                                       & = r
    \end{align*}
    The infinitesimal area element \(\odif{A}\) is therefore
    \begin{equation*}
        \odif{A} = \abs*{\det{\left( \symbf{J} \right)}} \odif{r}\odif{\theta} = r \odif{r}\odif{\theta}.
    \end{equation*}
\end{definition}
Using polar coordinates, the double integral over a region \(R\) is
defined
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} = \int_0^\infty \int_0^{2\pi} f\left( r \cos{\theta},\: r \sin{\theta} \right) r \odif{\theta} \odif{r}
\end{equation*}
\subsection{Strategies for Evaluating Double Integrals}
\begin{enumerate}
    \item Decompose the region of integration into simple domains, and
          if this is not possible, divide the region into subregions.
    \item Change the variables of integration to simplify the integral.
    \item Swap the order of integration to simplify the integral.
\end{enumerate}
\section{Multiple Integrals}
All results and properties for double integrals can be extended to
triple integrals.
\begin{theorem}[Fubini's Theorem in \(n\)-dimensions]
    If \(f : \R^n \to \R^+\), or \(f : \R^n \to \R\) is integrable, then
    all \(n! \) permutations of integrals are equal:
    \begin{equation*}
        \int_{\R^n} f\left( x_1,\: \ldots,\: x_n \right) \odif{x_1} \cdots \odif{x_n} = \int_{-\infty}^\infty \left[ \cdots \left[ \int_{-\infty}^\infty f\left( x_1,\: \ldots,\: x_n \right) \odif{x_1} \right] \cdots \right] \odif{x_n}
    \end{equation*}
\end{theorem}
As with double integrals, we can evaluate integrals of nonpositive
functions by using the positive and negative parts of the function.
\subsection{Vector-Valued Functions}
\begin{definition}[Vector-Valued Function]
    A vector-valued function \(\symbf{f} : \R^n \to \R^m\) is a
    function that maps a vector \(\symbf{x} \in \R^n\) to a vector
    \(\symbf{f}\left( \symbf{x} \right) \in \R^m\).
\end{definition}
A vector-valued function is integrable if either:
\begin{itemize}
    \item the norm of the function is integrable
          \begin{equation*}
              \int_{\R^n} \norm{\symbf{f}\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty
          \end{equation*}
    \item or if all components of the function are integrable
          \begin{equation*}
              \int_{\R^n} \abs*{f_i\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty \qquad \forall i \in \left\{ 1,\: \ldots,\: m \right\}
          \end{equation*}
\end{itemize}
The integral of a vector-valued function is defined as
\begin{equation*}
    \int_{\R^n} \symbf{f}\left( \symbf{x} \right) \odif{\symbf{x}} = \abracket*{\int_{\R^n} f_1\left( \symbf{x} \right) \odif{\symbf{x}},\: \ldots,\: \int_{\R^n} f_m\left( \symbf{x} \right) \odif{\symbf{x}}}
\end{equation*}
\subsection{Change of Variables}
\subsubsection{Cylindrical Coordinate Transformation}
\begin{definition}[Cylindrical Coordinates]
    A cylindrical coordinate system is defined by the \linebreak transformation
    \begin{align*}
        x & = r \cos{\theta} \\
        y & = r \sin{\theta} \\
        z & = z
    \end{align*}
    where \(r > 0\) is the radius of the cylinder,
    \(\theta \in \rinterval{0}{2\pi}\) is the angle of rotation
    measured anticlockwise from the positive \(x\)-axis, and \(z \in \R\)
    is the height of the cylinder.
    The Jacobian is given by
    \begin{align*}
        \symbf{J}                      & =
        \begin{bmatrix}
            \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\theta} & \displaystyle\pdv{x}{z} \\
            \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\theta} & \displaystyle\pdv{y}{z} \\
            \displaystyle\pdv{z}{r} & \displaystyle\pdv{z}{\theta} & \displaystyle\pdv{z}{z} \\
        \end{bmatrix}
        \\
                                       & =
        \begin{bmatrix}
            \cos{\theta} & -r \sin{\theta} & 0 \\
            \sin{\theta} & r \cos{\theta}  & 0 \\
            0            & 0               & 1 \\
        \end{bmatrix}
        \\
        \det{\left( \symbf{J} \right)} & = r \cos^2{\theta} + r \sin^2{\theta} \\
                                       & = r
    \end{align*}
    The infinitesimal area element \(\odif{V}\) is therefore
    \begin{equation*}
        \odif{V} = \abs*{\det{\left( \symbf{J} \right)}} \odif{r}\odif{\theta}\odif{z} = r \odif{r}\odif{\theta}\odif{z}.
    \end{equation*}
\end{definition}
Using cylindrical coordinates, the triple integral over a region \(R\) is
defined
\begin{equation*}
    \iint_R f\left( x,\: y,\: z \right) \odif{V} = \int_{-\infty}^{\infty} \int_0^\infty \int_0^{2\pi} f\left( r \cos{\theta},\: r \sin{\theta},\: z \right) r \odif{\theta} \odif{r} \odif{z}
\end{equation*}
\subsubsection{Spherical Coordinate Transformation}
\begin{definition}[Spherical Coordinates]
    A spherical coordinate system is defined by the \linebreak transformation
    \begin{align*}
        x & = \rho \sin{\phi} \cos{\theta} \\
        y & = \rho \sin{\phi} \sin{\theta} \\
        z & = \rho \cos{\phi}
    \end{align*}
    where \(\rho > 0\) is the radius of the sphere,
    \(\phi \in \rinterval{0}{\pi}\) is the polar angle
    measured down from the positive \(z\)-axis,
    and \(\theta \in \rinterval{0}{2\pi}\) is the azimuthal angle measured
    anticlockwise from the positive \(x\)-axis.
    The Jacobian is given by
    \begin{align*}
        \symbf{J}                      & =
        \begin{bmatrix}
            \displaystyle\pdv{x}{\rho} & \displaystyle\pdv{x}{\phi} & \displaystyle\pdv{x}{\theta} \\
            \displaystyle\pdv{y}{\rho} & \displaystyle\pdv{y}{\phi} & \displaystyle\pdv{y}{\theta} \\
            \displaystyle\pdv{z}{\rho} & \displaystyle\pdv{z}{\phi} & \displaystyle\pdv{z}{\theta} \\
        \end{bmatrix}
        \\
                                       & =
        \begin{bmatrix}
            \sin{\phi} \cos{\theta} & \rho \cos{\phi} \cos{\theta} & -\rho \sin{\phi} \sin{\theta} \\
            \sin{\phi} \sin{\theta} & \rho \cos{\phi} \sin{\theta} & \rho \sin{\phi} \cos{\theta}  \\
            \cos{\phi}              & -\rho \sin{\phi}             & 0                             \\
        \end{bmatrix}
        \\
        \det{\left( \symbf{J} \right)} & = \rho^2 \sin^3{\phi} \cos^2{\theta} + \rho^2 \sin{\phi} \cos^2{\phi} \cos^2{\theta} - \rho \sin{\phi} \sin{\theta} \left( -\rho \sin^2{\phi} \sin{\theta} - \rho \cos^2{\phi} \sin{\theta}  \right) \\
                                       & = \rho^2 \sin{\phi} \cos^2{\theta} \left( \sin^2{\phi} + \cos^2{\phi} \right) + \rho^2 \sin{\phi} \sin^2{\theta} \left( \sin^2{\phi} + \cos^2{\phi} \right)                                          \\
                                       & = \rho^2 \sin{\phi} \left( \sin^2{\theta} + \cos^2{\theta} \right)                                                                                                                                   \\
                                       & = \rho^2 \sin{\phi}
    \end{align*}
    The infinitesimal area element \(\odif{V}\) is therefore
    \begin{equation*}
        \odif{V} = \abs*{\det{\left( \symbf{J} \right)}} \odif{\rho}\odif{\phi}\odif{\theta} = \rho^2 \sin{\phi} \odif{\rho}\odif{\phi}\odif{\theta}.
    \end{equation*}
\end{definition}
Using spherical coordinates, the triple integral over a region \(R\) is
defined
\begin{equation*}
    \iiint_R f\left( x,\: y,\: z \right) \odif{V} = \int_0^\infty \int_0^{2\pi} \int_0^\pi f\left( \rho \sin{\phi} \cos{\theta},\: \rho \sin{\phi} \sin{\theta},\: \rho \cos{\phi} \right) \rho^2 \sin{\phi} \odif{\phi} \odif{\theta} \odif{\rho}
\end{equation*}
\subsection{Interpretations of Integrals}
\subsubsection*{Measure}
The measure of a region \(R \in \R^n\) is given by:
\begin{equation*}
    \mu\left( R \right) = \int_R \odif{\symbf{x}}
\end{equation*}
In two dimensions, the measure is the area of the region:
\begin{equation*}
    \mu\left( R \right) = \iint_R \odif{A}
\end{equation*}
In three dimensions, the measure is the volume of the region:
\begin{equation*}
    \mu\left( R \right) = \iiint_R \odif{V}
\end{equation*}
\subsubsection*{Mass}
The mass of a region \(R \in \R^n\) with density function \(\rho\) is
given by:
\begin{equation*}
    M = \int_R \rho\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Centroid}
The average position of a region \(R \in \R^n\) with uniform density is
given by:
\begin{equation*}
    \abracket*{\symbf{r}} = \frac{1}{\mu\left( R \right)} \int_R \symbf{x} \odif{\symbf{x}}
\end{equation*}
This point is the geometric centre of the region where the region would
balance if it were made of a uniform material.
\subsubsection*{Centre of Mass}
The average position of a region \(R \in \R^n\) with density function
\(\rho\) is given by:
\begin{equation*}
    \abracket*{\symbf{r}}_\rho = \frac{1}{M} \int_R \rho\left( \symbf{x} \right) \symbf{x} \odif{\symbf{x}}
\end{equation*}
This point is the centre of mass of the region where the region would
balance if it were made of a material with density \(\rho\).
\subsubsection*{Average Value}
The average value of a function \(f : \R^n \to \R\) over a region \(R
\in \R^n\) is given by:
\begin{equation*}
    \abracket*{f\left( \symbf{r} \right)} = \frac{1}{\mu\left( R \right)} \int_R f\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Expected Value}
The average value of a function \(f : \R^n \to \R\) over a region \(R
\in \R^n\) with density function \(p\) is given by:
\begin{equation*}
    \abracket*{f\left( \symbf{r} \right)}_p = \int_R p\left( \symbf{x} \right) f\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
where \(p\) is a probability density function satisfying
\begin{equation*}
    \int_R p\left( \symbf{x} \right) \odif{\symbf{x}} = 1
\end{equation*}
This result is the expected value of \(f\) over \(R\) if the region
is randomly sampled according to the probability density function \(p\).
\subsection{Properties of Integrals}
\subsubsection*{Linearity of Integrals}
If \(f\) and \(g\) are integrable functions, and \(c \in \R\), then
\begin{equation*}
    \int_{\R^n} \left( f\left( \symbf{x} \right) + g\left( \symbf{x} \right) \right) \odif{\symbf{x}} = \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} + \int_{\R^n} g\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
and
\begin{equation*}
    \int_{\R^n} c f\left( \symbf{x} \right) \odif{\symbf{x}} = c \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Positivity of Integrals}
If \(f \geqslant 0\) for all \(\symbf{x} \in \R^n\), then
\begin{equation*}
    \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} \geqslant 0
\end{equation*}
\subsubsection*{Monotonicity of Integrals}
If \(f\) and \(g\) are integrable functions, and \(f\left( \symbf{x}
\right) \leqslant g\left( \symbf{x} \right)\) for all \(\symbf{x} \in
\R^n\), then
\begin{equation*}
    \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} \leqslant \int_{\R^n} g\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Triangle Inequality}
If \(f\) is an integrable function, then
\begin{equation*}
    \norm*{\int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}}} \leqslant \int_{\R^n} \norm*{f\left( \symbf{x} \right)} \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Change of Variables}
When \(f\) is positive or integrable, the bijective transformation
\(\symbf{T} : R \subset \R^n \to R' \subset \R^n\) with continuous
first derivative \(\symbf{T}'\) allows us to change the variables of
integration from \(\symbf{x}\) to \(\symbf{u}\):
\begin{equation*}
    \int_{R} f\left( \symbf{x} \right) \odif{\symbf{x}} = \int_{R'} f\left( \symbf{T}\left( \symbf{u} \right) \right) \abs*{\det{\left( \symbf{J} \right)}} \odif{\symbf{u}}
\end{equation*}
where \(\symbf{J}\) is the Jacobian matrix of \(\symbf{T}\).
\subsubsection*{Measure Zero Integrals}
If \(\mu\left( R \right) = \int_R \odif{\symbf{x}} = 0\), then
\begin{equation*}
    \int_R f\left( \symbf{x} \right) \odif{\symbf{x}} = 0
\end{equation*}
\subsubsection*{Almost Equal Functions}
If \(f\) and \(g\) are integrable functions, and \(f\left( \symbf{x}
\right) = g\left( \symbf{x} \right)\) for all \(\symbf{x} \in \R^n\)
except for a set of measure zero, then
\begin{equation*}
    \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} = \int_{\R^n} g\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\section{Vector Calculus}
A vector-valued multivariable function is a function \(\symbfit{f} : E
\subset \R^n \to \R^m\) that maps a vector \(\symbf{x} \in \R^n\) to
the vector \(\symbfit{f}\left( \symbf{x} \right) \in \R^m\). \(f\) is
continuous if all components of \(\symbfit{f}\) are continuous, and
differentiable if all components of \(\symbfit{f}\) are differentiable.

If \(\symbfit{f}\) is differentiable at a point \(\symbf{x}_0\), then
it's derivative \(\symbfit{f}'\left( \symbf{x}_0 \right)\) is uniquely
defined by the Jacobian matrix of \(\symbfit{f}\) at \(\symbf{x}_0\):
\begin{equation*}
    \symbfit{f}'\left( \symbf{x}_0 \right) = \symbf{\nabla} \symbfit{f}\left( \symbf{x}_0 \right) = \symbf{J}^\top \left( \symbf{x}_0 \right)
\end{equation*}
\begin{definition}[Scalar Field]
    If \(m = 1\), then \(f : E \subset \R^n \to \R\) is a scalar field
    that associates a scalar value to each point \(\symbf{x} \in E\).
\end{definition}
\begin{definition}[Vector Field]
    If \(m = n\), then \(\symbf{F} : E \subset \R^n \to \R^n\) is a
    vector field that associates a vector to each point \(\symbf{x} \in E\).
    In this case, the Jacobian matrix is a square matrix and the
    Jacobian may be defined.
\end{definition}
\begin{definition}[Field Lines]
    The field lines of a vector field \(\symbf{F}\) are a family of
    curves that are tangent to \(\symbf{F}\) for all \(\symbf{x}\). They
    are defined as the solutions to the differential equation
    \begin{equation*}
        \symbf{r}'\left( t \right) = \symbf{F}\left( \symbf{r}\left( t \right) \right)
    \end{equation*}
    for all \(t\). When \(m = 2\),
    \begin{align*}
        \odv{x}{t} & = F_1\left( x,\: y \right) \\
        \odv{y}{t} & = F_2\left( x,\: y \right)
    \end{align*}
    and a field line may be defined in Cartesian coordinates as the
    solution to
    \begin{equation*}
        \odv{y}{x} = \frac{F_2\left( x,\: y \right)}{F_1\left( x,\: y \right)}.
    \end{equation*}
\end{definition}
\begin{lemma}
    The field lines of a scalar multiple of a vector field are the same
    as the field lines of that vector field.
\end{lemma}
\subsection{Differential Operators on Scalar Fields}
\begin{definition}[Gradient]
    The gradient of a scalar field is defined as the derivative of the
    scalar field in every direction:
    \begin{equation*}
        \grad{f} = \symbf{\nabla} f = \symbf{J}^\top =
        \begin{bmatrix}
            \pdv{f}{\symbf{x}_1} \\
            \vdots               \\
            \pdv{f}{\symbf{x}_n}
        \end{bmatrix}
    \end{equation*}
    The gradient measures the rate of change of a scalar field in all
    directions at a given point.
\end{definition}
\begin{definition}[Laplacian]
    The Laplacian of a scalar field is defined as the divergence of the
    gradient:
    \begin{equation*}
        \symbf{\Delta} f = \divergence{\left( \grad{f} \right)} = \symbf{\nabla}^2 f = \symbf{\nabla} \cdot \symbf{\nabla} f = \sum_{i = 1}^n \pdv[order=2]{f}{\symbf{x}_i}
    \end{equation*}
    The Laplacian measures the curvature or convexity of the surface
    \(z = f\left( \symbf{x} \right)\).
\end{definition}
\subsection{Differential Operators on Vector Fields}
\begin{definition}[Divergence]
    The divergence of a vector field is defined as the dot product of
    the gradient and the vector field:
    \begin{equation*}
        \divergence{\symbf{F}} = \symbf{\nabla} \cdot \symbf{F} = \pdv{\symbf{F}_1}{\symbf{x}_1} + \cdots + \pdv{\symbf{F}_n}{\symbf{x}_n} = \sum_{i = 1}^n \pdv{\symbf{F}_i}{\symbf{x}_i}.
    \end{equation*}
    Divergence measures the expansion of a vector field at a given point.
\end{definition}
\begin{itemize}
    \item \(\divergence{\symbf{F}}\left( \symbf{x}_0 \right) > 0\)
          indicates that \(\symbf{x}_0\) is a source, and the vector field is
          diverging out from \(\symbf{x}_0\).
    \item \(\divergence{\symbf{F}}\left( \symbf{x}_0 \right) < 0\)
          indicates that \(\symbf{x}_0\) is a sink, and the vector field is
          converging into \(\symbf{x}_0\).
    \item \(\divergence{\symbf{F}}\left( \symbf{x}_0 \right) = 0\)
          indicates that the net flow of the vector field at \(\symbf{x}_0\) is zero.
\end{itemize}
\begin{definition}[Curl]
    The curl of a vector field is defined as the cross product of the
    gradient and the vector field:
    \begin{equation*}
        \curl{\symbf{F}} = \symbf{\nabla} \times \symbf{F} =
        \begin{vmatrix}
            \symbf{i}                        & \symbf{j}                        & \symbf{k}                        \\
            \displaystyle\pdv{}{\symbf{x}_1} & \displaystyle\pdv{}{\symbf{x}_2} & \displaystyle\pdv{}{\symbf{x}_3} \\
            \symbf{F}_1                      & \symbf{F}_2                      & \symbf{F}_3                      \\
        \end{vmatrix}
        =
        \begin{vmatrix}
            \displaystyle\pdv{}{\symbf{x}_2} & \displaystyle\pdv{}{\symbf{x}_3} \\
            \symbf{F}_2                      & \symbf{F}_3                      \\
        \end{vmatrix}
        \symbf{i}
        -
        \begin{vmatrix}
            \displaystyle\pdv{}{\symbf{x}_1} & \displaystyle\pdv{}{\symbf{x}_3} \\
            \symbf{F}_1                      & \symbf{F}_3                      \\
        \end{vmatrix}
        \symbf{j}
        +
        \begin{vmatrix}
            \displaystyle\pdv{}{\symbf{x}_1} & \displaystyle\pdv{}{\symbf{x}_2} \\
            \symbf{F}_1                      & \symbf{F}_2                      \\
        \end{vmatrix}
        \symbf{k}
    \end{equation*}
    Curl measures the rotation of a vector field at a given point.
    \begin{itemize}
        \item \(\curl{\symbf{F}}\left( \symbf{x}_0 \right) > 0\)
              indicates that the vector field is rotating anticlockwise about
              \(\symbf{x}_0\).
        \item \(\curl{\symbf{F}}\left( \symbf{x}_0 \right) < 0\)
              indicates that the vector field is rotating clockwise about
              \(\symbf{x}_0\).
        \item \(\curl{\symbf{F}}\left( \symbf{x}_0 \right) = 0\)
              indicates that the net rotation about \(\symbf{x}_0\) is zero.
    \end{itemize}
\end{definition}
\subsection{Conservative Fields}
A vector field \(\symbf{F}\) is conservative if it is the gradient of a
potential function \(\phi\):
\begin{equation*}
    \symbf{F} = \symbf{\nabla} \phi.
\end{equation*}
Such a vector field represents a force field in which the total energy is
conserved.

The contours of the scalar field \(\phi\) are called equipotential
lines (\(\phi\) is constant). These lines are perpendicular to the
field lines of \(\symbf{F}\).
\begin{equation*}
    \left( \symbf{F} = \symbf{\nabla} \phi \right) \perp \left( \phi = \text{constant} \right)
\end{equation*}
This is because the contours of a scalar field are defined to be
perpendicular to the gradient of that scalar field.
\subsection{Line Integrals}
A line integral is an integral where the function to be integrated is
evaluated along a curve. This function may be a scalar field or a
vector field. Line integrals can be interpreted as a measure of the
total effect of a function along a curve.

To evaluate line integrals, we must define a parametrisation of the arc
length of the curve.
\subsubsection{Arc Length}
Arc length is the distance travelled along a path or curve. When
\(\symbf{r}\left( t \right)\) is a path, the arc length between
\(\symbf{r}\left( a \right)\) to \(\symbf{r}\left( t \right)\) is given
by
\begin{equation*}
    s\left( t \right) = \int_a^t \norm*{\symbf{r}'\left( \tau \right)} \odif{\tau}
\end{equation*}
where the integrand can be interpreted as the product of the speed of
along the path \(\symbf{r}\left( t \right)\) with a small time interval
\(\odif{\tau}\).
The length \(L\) of a path is therefore
\begin{equation*}
    L = s\left( b \right) = \int_a^b \norm*{\symbf{r}'\left( \tau \right)} \odif{\tau}.
\end{equation*}
\begin{theorem}[Arc Length Reparametrisation]
    A (piecewise) regular curve \(\mathscr{C}\) can always be
    reparametrised by the arc length parametrisation \(s\left( t \right)\):
    \begin{equation*}
        \tilde{\symbf{r}}\left( s \right) = \symbf{r}\left( \theta\left( s \right) \right)
    \end{equation*}
    for \(s \in \interval{0}{L}\).
\end{theorem}
\begin{proof}
    As \(r'\left( t \right) \neq 0\), \(\norm*{r'\left( t \right)} > 0\),
    and therefore, \(s\left( t \right)\) is a strictly increasing function.
    This means that \(s\left( t \right)\) is invertible, and therefore,
    \(t = \theta\left( s \right)\) is a bijective map between \(\interval{0}{L}\)
    and \(\interval{a}{b}\).
\end{proof}
\begin{remark}
    The rate of change of \(s\left( t \right)\) is the speed of the
    path \(\symbf{r}\left( t \right)\):
    \begin{equation*}
        \odv{s}{t} = \odv{}{t} \left[ \int_a^t \norm*{\symbf{r}'\left( \tau \right)} \odif{\tau} \right] = \norm*{\symbf{r}'\left( t \right)}.
    \end{equation*}
\end{remark}
\begin{remark}
    The speed of a path parametrised by arc length is always \(1\):
    \begin{equation*}
        \norm*{\tilde{\symbf{r}}'\left( s \right)} = \norm*{\symbf{r}'\left( \theta\left( s \right) \right)} \abs*{\odv{\theta\left( s \right)}{s}} = \norm*{\symbf{r}'\left( \theta\left( s \right) \right)} \frac{1}{\abs*{\odv{s}{\theta\left( s \right)}}} = \norm*{\symbf{r}'\left( \theta\left( s \right) \right)} \frac{1}{\norm*{\symbf{r}'\left( \theta\left( s \right) \right)}} = 1.
    \end{equation*}
\end{remark}
\subsubsection{Line Integral of a Scalar Field}
Let \(\mathscr{C}\) be a simple and piecewise regular curve
parametrised by \(\symbf{r}\left( t \right)\) with \(t \in
\interval{a}{b}\). The line integral of a scalar field \(f\) along
\(\mathscr{C}\) is defined as
\begin{equation*}
    \int_{\mathscr{C}} f \odif{s} = \int_a^b f\left( \symbf{r}\left( t \right) \right) \norm*{\symbf{r}'\left( t \right)} \odif{t}
\end{equation*}
where \(\odif{s} = \norm*{\symbf{r}'\left( t \right)} \odif{t}\) is the
differential arc length element along \(\mathscr{C}\).

This integral represents the weighted sum of \(f\) along
\(\mathscr{C}\), where the weight is the speed of the path
\(\symbf{r}\left( t \right)\). This line integral can be interpreted as
the area under the surface defined by \(f\) along \(\mathscr{C}\).
\begin{lemma}[Equivalence of Parametrisations]
    Let \(\tilde{\symbf{r}}\left( u \right)\) be a
    reparametrisation of \(\symbf{r}\left( t \right)\) with \(u \in
    \interval{c}{d}\). Then
    \begin{equation*}
        \int_{\mathscr{C}} f \odif{s} = \int_c^d f\left( \tilde{\symbf{r}}\left( u \right) \right) \norm*{\tilde{\symbf{r}}'\left( u \right)} \odif{u}
    \end{equation*}
    so that the line integral of a scalar field is independent of the
    parametrisation of \(\mathscr{C}\).
\end{lemma}
\begin{proof}
    Consider the parametrisation \(t = \theta\left( u \right)\),
    where the differential time element
    \(\odif{t} = \abs*{\theta'\left( u \right)} \odif{u}\), by the
    Change of Variables property. Then
    \begin{align*}
        \int_{\mathscr{C}} f \odif{s} & = \int_a^b f\left( \symbf{r}\left( t \right) \right) \norm*{\symbf{r}'\left( t \right)} \odif{t}                                                                          \\
                                      & = \int_c^d f\left( \symbf{r}\left( \theta\left( u \right) \right) \right) \norm*{\symbf{r}'\left( \theta\left( u \right) \right)} \abs*{\theta'\left( u \right)} \odif{u} \\
                                      & = \int_c^d f\left( \tilde{\symbf{r}}\left( u \right) \right) \norm*{\tilde{\symbf{r}}'\left( u \right)} \odif{u}.
    \end{align*}
\end{proof}
\subsubsection{Line Integral of a Vector Field}
Let \(\mathscr{C}\) be a simple and piecewise regular curve
parametrised by \(\symbf{r}\left( t \right)\) with \(t \in
\interval{a}{b}\). The line integral of a vector field \(\symbf{F}\)
along \(\mathscr{C}\) is defined as
\begin{equation*}
    \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} = \int_a^b \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t}
\end{equation*}
where \(\odif{\symbf{r}} = \symbf{r}'\left( t \right) \odif{t}\) is the
differential path element along \(\mathscr{C}\).

This integral represents the weighted sum of \(\symbf{F}\) along
\(\mathscr{C}\), where the weight is the component of the velocity of
the path \(\symbf{r}\left( t \right)\) in the direction of
\(\symbf{F}\). This line integral can be interpreted as the work done
by the force field \(\symbf{F}\) along \(\mathscr{C}\).
\begin{lemma}[Equivalence of Parametrisations]
    Let \(\tilde{\symbf{r}}\left( u \right)\) be a
    reparametrisation of \(\symbf{r}\left( t \right)\) with \(u \in
    \interval{c}{d}\). Then
    \begin{equation*}
        \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} =
        \begin{cases}
            \displaystyle\int_c^d \symbf{F}\left( \tilde{\symbf{r}}\left( u \right) \right) \cdot \tilde{\symbf{r}}'\left( u \right) \odif{u}  & \text{if } \theta'\left( u \right) > 0 \\[2.5ex]
            -\displaystyle\int_c^d \symbf{F}\left( \tilde{\symbf{r}}\left( u \right) \right) \cdot \tilde{\symbf{r}}'\left( u \right) \odif{u} & \text{if } \theta'\left( u \right) < 0
        \end{cases}
    \end{equation*}
    so that the line integral of a vector field is independent of the
    parametrisation of \(\mathscr{C}\).
\end{lemma}
\begin{proof}
    Consider the parametrisation \(t = \theta\left( u \right)\),
    where the differential time element
    \(\odif{t} = \abs*{\theta'\left( u \right)} \odif{u}\), by the
    Change of Variables property. Then
    \begin{align*}
        \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} & = \int_a^b \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t} \\
                                                            & =
        \begin{cases}
            \displaystyle\int_c^d \symbf{F}\left( \symbf{r}\left( \theta\left( u \right) \right) \right) \cdot \symbf{r}'\left( \theta\left( u \right) \right) \abs*{\theta'\left( u \right)} \odif{u}  & \text{if } \theta'\left( u \right) > 0 \\[2.5ex]
            -\displaystyle\int_c^d \symbf{F}\left( \symbf{r}\left( \theta\left( u \right) \right) \right) \cdot \symbf{r}'\left( \theta\left( u \right) \right) \abs*{\theta'\left( u \right)} \odif{u} & \text{if } \theta'\left( u \right) < 0
        \end{cases}
        \\
                                                            & =
        \begin{cases}
            \displaystyle\int_c^d \symbf{F}\left( \tilde{\symbf{r}}\left( u \right) \right) \cdot \tilde{\symbf{r}}'\left( u \right) \odif{u}  & \text{if } \theta'\left( u \right) > 0 \\[2.5ex]
            -\displaystyle\int_c^d \symbf{F}\left( \tilde{\symbf{r}}\left( u \right) \right) \cdot \tilde{\symbf{r}}'\left( u \right) \odif{u} & \text{if } \theta'\left( u \right) < 0
        \end{cases}
    \end{align*}
\end{proof}
\begin{corollary}[Line Integrals in the Reverse Direction]
    Taking the line integral of a path in the reverse direction is
    equivalent to negating the line integral over the original path.
    \begin{align*}
        \int_{-\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} & = -\int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}}
    \end{align*}
    where \(-\mathscr{C}\) is the curve parametrised by
    \(\symbf{r}\left( a + b - t \right)\).
\end{corollary}
\subsubsection{Relationship between Line Integrals over Scalar and Vector Fields}
Line integrals over scalar fields may be evaluated with respect to a
single variable \(x_i\):
\begin{equation*}
    \int_{\mathscr{C}} f \odif{x_i} = \int_a^b f\left( \symbf{r}\left( t \right) \right) \symbf{r}'_i\left( t \right) \odif{t}
\end{equation*}
where \(\odif{x_i} = \symbf{r}'_i\left( t \right) \odif{t}\) is the
differential element in the direction of \(x_i\) along \(\mathscr{C}\).
By adding the line integrals over all \(x_i\), we obtain
\begin{equation*}
    \int_{\mathscr{C}} \left( \symbf{F}_1 \odif{x_1} + \cdots + \symbf{F}_n \odif{x_n} \right) = \int_a^b \left( \symbf{F}_1 \symbf{r}'_1 \odif{t} + \cdots + \symbf{F}_n \symbf{r}'_n \odif{t} \right) = \int_a^b \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t} = \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}}.
\end{equation*}
Line integrals over vector fields may be evaluated using the unit
tangent vector of the path \(\symbf{r}\left( t \right)\), so that
\begin{equation*}
    \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} = \int_a^b \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \frac{\symbf{r}'\left( t \right)}{\norm*{\symbf{r}'\left( t \right)}} \norm*{\symbf{r}'\left( t \right)} \odif{t} = \int_a^b \underbrace{\symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \hat{\symbf{\tau}}\left( t \right)}_f \norm*{\symbf{r}'\left( t \right)} \odif{t} = \int_{\mathscr{C}} f \odif{s}.
\end{equation*}
\subsubsection{Line Integrals over Multiple Paths}
Let \(\mathscr{C}_1\) and \(\mathscr{C}_2\) be two simple and piecewise
regular curves parametrised by \(\symbf{r}_1 : \interval{a}{b} \subset
\R \to \R^n\) and \(\symbf{r}_2 : \interval{b}{c} \subset \R \to \R^n\)
respectively, such that \(\symbf{r}_1\left( b \right) =
\symbf{r}_2\left( b \right)\). The line integrals over the
concatenation of \(\mathscr{C}_1\) and \(\mathscr{C}_2\) are defined as
\begin{align*}
    \int_{\mathscr{C}_1 + \mathscr{C}_2} f \odif{s}                       & = \int_{\mathscr{C}_1} f \odif{s} + \int_{\mathscr{C}_2} f \odif{s}                                              \\
    \int_{\mathscr{C}_1 + \mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}} & = \int_{\mathscr{C}_1} \symbf{F} \cdot \odif{\symbf{r}} + \int_{\mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}}.
\end{align*}
\subsubsection{Fundamental Theorem of Line Integrals}
Let \(\symbf{F} = \symbf{\nabla} \phi\) be a conservative vector field
and \(\mathscr{C}\) be a simple and piecewise regular curve
parametrised by \(\symbf{r}\left( t \right)\) with \(t \in
\interval{a}{b}\). Then
\begin{equation*}
    \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} = \int_{\mathscr{C}} \symbf{\nabla} \phi \cdot \odif{\symbf{r}} = \phi\left( \symbf{b} \right) - \phi\left( \symbf{a} \right)
\end{equation*}
where \(\symbf{a} = \symbf{r}\left( a \right)\) and \(\symbf{b} = \symbf{r}\left( b \right)\).
This result demonstrates that a line integral in a conservative field
is path independent, and depends only on the endpoints of the path.
\subsubsection{Circulation}
Let \(\mathscr{C}\) be a simple closed curve parametrised by
\(\symbf{r}\left( t \right)\) with \(t \in \interval{a}{b}\), where
\(\symbf{r}\left( a \right) = \symbf{r}\left( b \right)\). The line
integral of a vector field \(\symbf{F}\) around \(\mathscr{C}\) is
called the circulation of \(\symbf{F}\) around \(\mathscr{C}\), and is
notated by a circle inside the integral sign:
\begin{equation*}
    \Gamma = \oint_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}}.
\end{equation*}
\subsubsection{Theorems in Conservative Fields}
Let \(\symbf{F}\) be a continuous conservative vector field in an open
and connected region \(\Omega \subset \R^n\). Then the following
theorems hold:
\begin{theorem}[Circulation in a Conservative Field]\label{thm:circulation_in_conservative_field}
    The circulation of \(\symbf{F}\) around any closed path
    \(\mathscr{C} \in \Omega\) is zero:
    \begin{equation*}
        \oint_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} = 0.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let \(\mathscr{C}\) be a closed path parametrised by
    \(\symbf{r}\left( t \right)\) with \(t \in \interval{a}{b}\), where
    \(\symbf{r}\left( a \right) = \symbf{r}\left( b \right)\). Then
    \begin{equation*}
        \oint_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}} = \oint_a^b \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t} = \oint_a^b \symbf{\nabla} \phi\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t} = \phi\left( \symbf{r}\left( b \right) \right) - \phi\left( \symbf{r}\left( a \right) \right) = 0.
    \end{equation*}
\end{proof}
\begin{theorem}[Path Independence in a Conservative Field]
    The line integral of \(\symbf{F}\) between two points \(\symbf{a}\)
    and \(\symbf{b}\) in \(\Omega\) is independent of the path connecting
    \(\symbf{a}\) and \(\symbf{b}\):
    \begin{equation*}
        \int_{\mathscr{C}_1} \symbf{F} \cdot \odif{\symbf{r}} = \int_{\mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let \(\mathscr{C}_1\) and \(\mathscr{C}_2\) be two different paths
    parametrised by \(\symbf{r}_1\left( t \right)\) and \(\symbf{r}_2\left( t \right)\)
    respectively, with \(t \in \interval{a}{b}\), where
    \(\symbf{r}_1\left( a \right) = \symbf{r}_2\left( a \right)\) and
    \(\symbf{r}_1\left( b \right) = \symbf{r}_2\left( b \right)\).
    Consider the opposite reparametrisation of \(\symbf{r}_2\left( t \right)\)
    such that the concatenation of \(\symbf{r}_1\) and \(\tilde{\symbf{r}}_2\)
    is a closed path. Then, by Theorem~\ref{thm:circulation_in_conservative_field},
    \begin{equation*}
        0 = \oint_{\mathscr{C}_1 - \mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}} = \int_{\mathscr{C}_1} \symbf{F} \cdot \odif{\symbf{r}} + \int_{-\mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}} = \int_{\mathscr{C}_1} \symbf{F} \cdot \odif{\symbf{r}} - \int_{\mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}}
    \end{equation*}
    which implies that
    \begin{equation*}
        \int_{\mathscr{C}_1} \symbf{F} \cdot \odif{\symbf{r}} = \int_{\mathscr{C}_2} \symbf{F} \cdot \odif{\symbf{r}}.
    \end{equation*}
\end{proof}
Using this theorem, it is possible to show that a conservative field
is the gradient of a potential function.
\begin{proof}[Proof for the Definition of a Conservative Field]
    Consider the line integral of \(\symbf{F}\) from a fixed point
    \(\symbf{x}_0\) to a variable point \(\symbf{x}\) in \(\Omega\):
    \begin{equation*}
        \phi\left( \symbf{x} \right) = \int_{\symbf{x}_0}^{\symbf{x}} \symbf{F} \cdot \odif{\symbf{r}}.
    \end{equation*}
    The derivative of \(\phi\left( \symbf{x} \right)\) with
    respect to \(\symbf{x}_i\) is given by
    \begin{align*}
        \pdv{\phi}{\symbf{x}_i} & = \lim_{h \to 0} \frac{1}{h} \left[ \phi\left( \symbf{x} + h \symbf{e}_i \right) - \phi\left( \symbf{x} \right) \right]                                                                       \\
                                & = \lim_{h \to 0} \frac{1}{h} \left[ \int_{\symbf{x}_0}^{\symbf{x} + h \symbf{e}_i} \symbf{F} \cdot \odif{\symbf{r}} - \int_{\symbf{x}_0}^{\symbf{x}} \symbf{F} \cdot \odif{\symbf{r}} \right] \\
                                & = \lim_{h \to 0} \frac{1}{h} \int_{\symbf{x}}^{\symbf{x} + h \symbf{e}_i} \symbf{F} \cdot \odif{\symbf{r}}
    \end{align*}
    The path of integration is a straight line from \(\symbf{x}\) to
    \(\symbf{x} + h \symbf{e}_i\), and can be parametrised by
    \(\symbf{r}\left( t \right) = \symbf{x} + t h \symbf{e}_i\) for
    \(t \in \interval{0}{1}\). Then
    \begin{align*}
        \pdv{\phi}{\symbf{x}_i} & = \lim_{h \to 0} \frac{1}{h} \int_0^1 \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t} \\
                                & = \lim_{h \to 0} \frac{1}{h} \int_0^1 \symbf{F}\left( \symbf{x} + t h \symbf{e}_i \right) \cdot h \symbf{e}_i \odif{t}            \\
                                & = \lim_{h \to 0} \int_0^1 \symbf{F}\left( \symbf{x} + t h \symbf{e}_i \right) \odif{t}
    \end{align*}
    As the integrand is a continuous function of \(t\) and \(\symbf{x}\),
    the mean value theorem for integrals implies that there exists a
    time \(t_0 \in \interval{0}{1}\) where the integral is equal to the
    mean value of \(\symbf{F}\) on \(\interval{0}{1}\):
    \begin{equation*}
        \pdv{\phi}{\symbf{x}_i} = \lim_{h \to 0} \symbf{F}\left( \symbf{x} + t_0 h \symbf{e}_i \right) \cdot \symbf{e}_i = \symbf{F}\left( \symbf{x} \right) \cdot \symbf{e}_i.
    \end{equation*}
    Without loss of generality, this can be extended to all \(i \in \interval{1}{n}\),
    so that:
    \begin{equation*}
        \symbf{F}\left( \symbf{x} \right) = \symbf{\nabla} \phi\left( \symbf{x} \right).
    \end{equation*}
\end{proof}
\begin{theorem}[Antiderivative of a Conservative Field]
    The line integral of \(\symbf{F}\) from a fixed point \(\symbf{x}_0\)
    to a variable point \(\symbf{x}\) in \(\Omega\) is precisely the
    potential function \(\phi\left( \symbf{x} \right)\) evaluated at
    \(\symbf{x}\):
    \begin{equation*}
        \phi\left( \symbf{x} \right) = \int_{\symbf{x}_0}^{\symbf{x}} \symbf{F} \cdot \odif{\symbf{r}}.
    \end{equation*}
\end{theorem}
\begin{remark}
    When \(\symbf{F}\) is not a conservative field, the line integral
    may still be evaluated, but the result will not be path independent.
\end{remark}
\begin{theorem}[Curl Criterion for Conservative Fields]
    For \(n = 2\) and \(n = 3\), if \(\symbf{F} \subset C^1\) is a
    continuous vector field in an open and simply-connected region
    \(\Omega \subset \R^n\), then
    \begin{equation*}
        \left( \exists \phi \subset C^2 : \symbf{F} = \symbf{\nabla}\phi \right) \iff \symbf{\nabla} \times \symbf{F} = \symbf{0}.
    \end{equation*}
\end{theorem}
\begin{proof}
    The forward direction uses Schwartz theorem:
    \begin{equation*}
        \left( \exists \phi \subset C^2 : \symbf{F} = \symbf{\nabla}\phi \right) \implies \symbf{\nabla} \times \symbf{F} = \symbf{\nabla} \times \symbf{\nabla} \phi =
        \begin{bmatrix}
            \partial_x \\
            \partial_y \\
            \partial_z
        \end{bmatrix}
        \times
        \begin{bmatrix}
            \phi_x \\
            \phi_y \\
            \phi_z
        \end{bmatrix}
        =
        \begin{bmatrix}
            \phi_{zy} - \phi_{yz}                 \\
            -\left( \phi_{zx} - \phi_{xz} \right) \\
            \phi_{yx} - \phi_{xy}
        \end{bmatrix}
        = \symbf{0}.
    \end{equation*}
\end{proof}
\subsubsection{Energy Theorems}
\begin{theorem}[Work Energy Theorem]
    The total work done by a field \(\symbf{F}\) on a particle moving
    along a curve \(\mathscr{C}\) from point \(\symbf{a}\) to \(\symbf{b}\)
    is equal to the change in kinetic energy \(T\) of the particle along
    \(\mathscr{C}\):
    \begin{equation*}
        W = T_{\symbf{b}} - T_{\symbf{a}}.
    \end{equation*}
\end{theorem}
\begin{proof}
    From Newton's Second Law
    \begin{align*}
        m \symbf{r}''\left( t \right)                                                                                        & = \symbf{F}\left( \symbf{r}\left( t \right) \right)                                                    \\
        m \symbf{r}'\left( t \right) \cdot \symbf{r}''\left( t \right)                                                       & = \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right)                   \\
        \int_a^b m \symbf{r}'\left( t \right) \cdot \symbf{r}''\left( t \right) \odif{t}                                     & = \int_a^b \symbf{F}\left( \symbf{r}\left( t \right) \right) \cdot \symbf{r}'\left( t \right) \odif{t} \\
        \int_a^b \odv*{\left[ \frac{1}{2} m \symbf{r}'\left( t \right) \cdot \symbf{r}'\left( t \right) \right]}{t} \odif{t} & = \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}}                                                  \\
        \int_a^b \odv*{\left[ \frac{1}{2} m \norm*{\symbf{v}\left( t \right)}^2 \right]}{t} \odif{t}                         & = \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}}                                                  \\
        \frac{1}{2} m \norm*{\symbf{v}\left( b \right)}^2 - \frac{1}{2} m \norm*{\symbf{v}\left( a \right)}^2                & = \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{r}}                                                  \\
        T_{\symbf{b}} - T_{\symbf{a}}                                                                                        & = W
    \end{align*}
\end{proof}
\begin{definition}[Total Energy]
    The total energy of a particle is given by \(E = T + V\), where \(V\)
    is the potential energy of conservative forces acting on the particle:
    \(\symbf{F}_{\text{c}} = -\symbf{\nabla} V\).
\end{definition}
\begin{theorem}[Conservation of Energy]
    The work done by non-conservative forces on a particle moving along
    a curve \(\mathscr{C}\) from point \(\symbf{a}\) to \(\symbf{b}\) is
    equal to the change in total energy of the particle along \(\mathscr{C}\):
    \begin{equation*}
        W_{\mathrm{nc}} = E_{\symbf{b}} - E_{\symbf{a}}.
    \end{equation*}
\end{theorem}
\begin{proof}
    From the Work Energy Theorem,
    \begin{align*}
        T_{\symbf{b}} - T_{\symbf{a}}                                                               & = W                                                                                                                             \\
        T_{\symbf{b}} - T_{\symbf{a}}                                                               & = \int_{\mathscr{C}} \left( \symbf{F}_{\text{c}} + \symbf{F}_{\text{nc}} \right) \cdot \odif{\symbf{r}}                         \\
        T_{\symbf{b}} - T_{\symbf{a}}                                                               & = \int_{\mathscr{C}} \left( -\symbf{\nabla} V + \symbf{F}_{\text{nc}} \right) \cdot \odif{\symbf{r}}                            \\
        T_{\symbf{b}} - T_{\symbf{a}}                                                               & = -\int_{\mathscr{C}} \symbf{\nabla} V \cdot \odif{\symbf{r}} + \int_{\mathscr{C}} \symbf{F}_{\text{nc}} \cdot \odif{\symbf{r}} \\
        T_{\symbf{b}} - T_{\symbf{a}}                                                               & = -\left( V_{\symbf{b}} - V_{\symbf{a}} \right) + \int_{\mathscr{C}} \symbf{F}_{\text{nc}} \cdot \odif{\symbf{r}}               \\
        \left( T_{\symbf{b}} + V_{\symbf{b}} \right) - \left( T_{\symbf{a}} + V_{\symbf{a}} \right) & = \int_{\mathscr{C}} \symbf{F}_{\text{nc}} \cdot \odif{\symbf{r}}                                                               \\
        E_{\symbf{b}} - E_{\symbf{a}}                                                               & = W_{\text{nc}}
    \end{align*}
\end{proof}
\subsection{Surface Integrals}
A surface integral is a multiple integral where the function to be
integrated is evaluated across a curved surface. This function may be a
scalar field or a vector field.
\subsubsection{Surface Area}
The surface area of a surface \(\mathscr{S}\) parametrised by
\(\symbf{r}\left( s, t \right)\) is given by
\begin{equation*}
    A = \iint_{\mathscr{S}} \odif{\sigma} = \iint_{\mathscr{S}} \norm*{\symbf{r}_s \times \symbf{r}_t} \odif{s} \odif{t}
\end{equation*}
where \(\norm*{\symbf{r}_s \times \symbf{r}_t}\) represents the area of
the parallelogram spanned by \(\symbf{r}_s\) and \(\symbf{r}_t\).
\subsubsection{Surface Integral over a Scalar Field}
Let \(\mathscr{S}\) be a surface parametrised by \(\symbf{r}\left( s, t
\right)\). The surface integral of a scalar field \(f\) over
\(\mathscr{S}\) is defined as
\begin{equation*}
    \iint_{\mathscr{S}} f \odif{\sigma} = \iint_{\mathscr{S}} f\left( \symbf{r}\left( s, t \right) \right) \norm*{\symbf{r}_s \times \symbf{r}_t} \odif{s} \odif{t}.
\end{equation*}
where \(\odif{\sigma} = \norm*{\symbf{r}_s \times \symbf{r}_t} \odif{s}\odif{t}\)
is the differential surface element of \(\mathscr{S}\).
This integral represents the weighted sum of \(f\) over \(\mathscr{S}\).
\begin{lemma}[Equivalence of Parametrisations]
    Let \(\mathscr{S}\) be a surface parametrised by \(\symbf{r}\left( s, t \right)\)
    and let \(\tilde{\symbf{r}}\left( u, v \right)\) be a reparametrisation
    of \(\symbf{r}\left( s, t \right)\). Then
    \begin{equation*}
        \iint_{\mathscr{S}} f \odif{\sigma} = \iint_{\mathscr{S}} f\left( \tilde{\symbf{r}}\left( u,\: v \right) \right) \norm*{\tilde{\symbf{r}}_u \times \tilde{\symbf{r}}_v} \odif{u} \odif{v}.
    \end{equation*}
    so that the surface integral of a scalar field is independent of the
    parametrisation of \(\mathscr{S}\).
\end{lemma}
\subsubsection{Surface Integral over a Vector Field}
Let \(\mathscr{S}\) be an orientable surface parametrised by
\(\symbf{r}\left( s, t \right)\). The surface (flux) integral of a
vector field \(\symbf{F}\) over \(\mathscr{S}\) is defined as the flux
\(\Phi\) of \(\symbf{F}\) through \(\mathscr{S}\):
\begin{equation*}
    \Phi =
    \iint_{\mathscr{S}} \symbf{F} \cdot \odif{\symbf{\sigma}} =
    \pm \iint_{\mathscr{S}} \symbf{F}\left( \symbf{r}\left( s, t \right) \right) \cdot \left( \symbf{r}_s \times \symbf{r}_t \right) \odif{s} \odif{t}.
\end{equation*}
where \(\hat{\symbf{n}}\) is the unit normal vector to \(\mathscr{S}\),
and \(\odif{\symbf{\sigma}} = \left( \symbf{r}_s \times \symbf{r}_t \right) \odif{s}\odif{t}\)
is the differential vector surface element of \(\mathscr{S}\), which is
related to the differential surface element \(\odif{\sigma}\) by
\begin{align*}
    \odif{\sigma}         & = \norm*{\odif{\symbf{\sigma}}} \\
    \odif{\symbf{\sigma}} & = \hat{\symbf{n}} \odif{\sigma}
\end{align*}
so that this integral may also be written as
\begin{equation*}
    \Phi =
    \iint_{\mathscr{S}} \symbf{F} \cdot \odif{\symbf{\sigma}} =
    \pm \iint_{\mathscr{S}} \symbf{F}\left( \symbf{r}\left( s, t \right) \right) \cdot \hat{\symbf{n}} \odif{\sigma}.
\end{equation*}
This integral represents the outward or inward flow of \(\symbf{F}\)
through \(\mathscr{S}\), where the direction of flow depends on the sign
of \(\hat{\symbf{n}}\).
\begin{lemma}[Equivalence of Parametrisations]
    Let \(\mathscr{S}\) be an orientable surface parametrised by \(\symbf{r}\left( s, t \right)\)
    and let \(\tilde{\symbf{r}}\left( u, v \right)\) be a reparametrisation
    of \(\symbf{r}\left( s, t \right)\). Then
    \begin{equation*}
        \iint_{\mathscr{S}} \symbf{F} \cdot \odif{\symbf{\sigma}} = \pm \iint_{\mathscr{S}} \symbf{F}\left( \tilde{\symbf{r}}\left( u,\: v \right) \right) \cdot \left( \tilde{\symbf{r}}_u \times \tilde{\symbf{r}}_v \right) \odif{u} \odif{v}.
    \end{equation*}
    so that the surface integral of a vector field is independent of the
    parametrisation of \(\mathscr{S}\).
\end{lemma}
\end{document}
