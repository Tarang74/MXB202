%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros

% Header and footer
\newcommand{\unitName}{Advanced Calculus}
\newcommand{\unitTime}{Semester 2, 2023}
\newcommand{\unitCoordinator}{Dr Pascal Buenzli}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Euclidean Space}
The Euclidean space \(\R^n\) is an \(n\)-dimensional vector space of
real numbers. This space is closed under addition and scalar
multiplication.
\subsection{Operations}
\subsubsection{Addition}
The sum of two vectors \(\symbf{x}\) and \(\symbf{y}\) is defined
element-wise
\begin{equation*}
    \symbf{x} + \symbf{y} =
    \begin{bmatrix}
        x_1 + y_1 \\
        x_2 + y_2 \\
        \vdots    \\
        x_n + y_n
    \end{bmatrix}
\end{equation*}
In a coordinate system, the vectors \(\symbf{x}\) and \(\symbf{y}\) are added tip-to-tail.
\subsubsection{Scalar Multiplication}
The scalar multiplication of a vector \(\symbf{x}\) by a scalar
\(\lambda \in \R\) is defined element-wise
\begin{equation*}
    \lambda \symbf{x} =
    \begin{bmatrix}
        \lambda x_1 \\
        \lambda x_2 \\
        \vdots      \\
        \lambda x_n
    \end{bmatrix}
\end{equation*}
In a coordinate system, \(\lambda\) scales the vector \(\symbf{x}\) along the same line.
\subsubsection{Norm}
The norm (length) of a vector \(\symbf{x}\) is defined as
\begin{equation*}
    \norm{\symbf{x}} = \sqrt{\symbf{x} \cdot \symbf{x}} = \sqrt{\sum_{i=1}^n x_i^2}
\end{equation*}
The norm of a vector \(\symbf{x}\) is the distance from the origin to the tip of the vector.
This allows us to define the unit vector \(\hat{\symbf{x}}\) as
\begin{equation*}
    \hat{\symbf{x}} = \frac{\symbf{x}}{\norm{\symbf{x}}}
\end{equation*}
which is a vector of length 1 in the same direction as \(\symbf{x}\).
\subsubsection{Scalar Product}
The scalar product (dot product) of two vectors \(\symbf{x}\) and
\(\symbf{y}\) is defined as
\begin{equation*}
    \symbf{x} \cdot \symbf{y} = \sum_{i=1}^n x_i y_i
\end{equation*}
The scalar product allows us to define the angle \(\theta\) between two vectors \(\symbf{x}\) and \(\symbf{y}\) as
\begin{equation*}
    \cos{\left( \theta \right)} = \hat{\symbf{x}} \cdot \hat{\symbf{y}}
\end{equation*}
where we use the unit vectors of \(\symbf{x}\) and \(\symbf{y}\), as the angle between two vectors is invariant under scaling.
Additionally, we can determine the projection of the vector \(\symbf{x}\) onto the vector \(\symbf{y}\) using trigonometry
\begin{equation*}
    \proj_{\symbf{y}} \left( \symbf{x} \right) = \left( \norm{\symbf{x}} \cos{\left( \theta \right)} \right) \hat{\symbf{y}} = \left( \norm{\symbf{x}} \left( \hat{\symbf{x}} \cdot \hat{\symbf{y}} \right) \right) \hat{\symbf{y}} = \left( \symbf{x} \cdot \hat{\symbf{y}} \right) \hat{\symbf{y}}
\end{equation*}
where \(\symbf{x} \cdot \hat{\symbf{y}}\) is the norm of the projection vector.
\subsection{Additional Properties}
\subsubsection{Triangle Inequality}
\begin{equation*}
    \norm{\symbf{x} + \symbf{y}} \leqslant \norm{\symbf{x}} + \norm{\symbf{y}}
\end{equation*}
\subsubsection{Inverse Triangle Inequality}
\begin{equation*}
    \norm{\symbf{x} - \symbf{y}} \geqslant \abs{\norm{\symbf{x}} - \norm{\symbf{y}}}
\end{equation*}
\subsubsection{Cauchy-Schwarz Inequality}
\begin{equation*}
    \abs{\symbf{x} \cdot \symbf{y}} \leqslant \norm{\symbf{x}} \norm{\symbf{y}}
\end{equation*}
\subsection{Multivariable Functions}
A multivariable function \(f\) maps a vector \(\symbf{x} \in \R^n\) to
a real number \(f \left( \symbf{x} \right) \in \R\). This function can
be expressed in \textbf{explicit form} as
\begin{equation*}
    z = f\left( x,\: y \right)
\end{equation*}
or in \textbf{implicit form} as
\begin{equation*}
    F\left( x,\: y,\: z \right) = z - f\left( x,\: y \right)
\end{equation*}
These equations define a surface in \(\R^3\).
\subsubsection{Level Curves}
The level curves of a function \(f \left( x,\: y \right)\) are the
curves in \(\R^2\) where
\begin{equation*}
    f \left( x,\: y \right) = c
\end{equation*}
where \(c\) is the height of the curve. Implicitly, this is equivalent to
\begin{equation*}
    F\left( x,\: y,\: z \right) = 0.
\end{equation*}
Level curves represent paths of equal height on the surface defined by \(z = f \left( x,\: y \right)\).
\subsection{Special Regions}
\subsubsection{Balls}
In an Euclidean space, an open ball of radius \(r > 0\) centred at a
point \(\symbf{p} \in \R^n\) is denoted \(B_r\left( \symbf{p}
\right)\), and is defined as
\begin{equation*}
    B_r\left( \symbf{p} \right) = \left\{ \symbf{x} \in \R^n : \norm{\symbf{x} - \symbf{p}} < r \right\}.
\end{equation*}
This region includes all points less than a distance \(r\) from the vector \(\symbf{p}\),
where the distance is typically defined by the \(L_2\)-norm:
\begin{equation*}
    \norm{\symbf{x} - \symbf{p}}_2 = \left( \sum_{i=1}^n \left( x_i - p_i \right)^2 \right)^{1/2}.
\end{equation*}
\subsection{Mathematical Representation of Curves}
\subsubsection{Explicit Form}
A curve in \(\R^2\) can be represented in explicit form as
\begin{equation*}
    y = f\left( x \right)
\end{equation*}
but this is not possible in \(\R^3\) as a 3D curve requires two equations.
For a 2D explicit curve:
\begin{itemize}
    \item \(x\) is an independent variable such that we have 1 degree of freedom.
\end{itemize}
\subsubsection{Implicit Form}
A curve in \(\R^2\) can be represented in implicit form as
\begin{equation*}
    F\left( x,\: y \right) = 0.
\end{equation*}
In 3D, we must impose an additional equation that intersects a surface.
\begin{equation*}
    \left\{
    \begin{aligned}
        F\left( x,\: y,\: z \right) & = 0 \\
        G\left( x,\: y,\: z \right) & = 0
    \end{aligned}
    \right.
\end{equation*}
In both cases, we have 1 degree of freedom as the degrees of freedom is the
difference between the number of variables and the number of equations.
\subsubsection{Parametric Form}
In parametric form, curves are parametrised in terms of a parameter
\(t\). In 2D, this is represented as
\begin{equation*}
    \symbf{r}\left( t \right) = \abracket*{x\left( t \right),\: y\left( t \right)}
\end{equation*}
and similarly in 3D,
\begin{equation*}
    \symbf{r}\left( t \right) = \abracket*{x\left( t \right),\: y\left( t \right),\: z\left( t \right)}
\end{equation*}
\subsection{Converting Between Representations}
\subsubsection{Explicit to Implicit}
The equation \(y = f\left( x \right)\) can always be converted to
implicit form by rewriting it as
\begin{equation*}
    F\left( x,\: y \right) = y - f\left( x \right) = 0.
\end{equation*}
\subsubsection{Implicit to Explicit}
The equation \(F\left( x,\: y \right) = 0\) can be converted to
explicit form if we can solve for \(y\) (or \(x\)):
\subsubsection{Parametric to Explicit/Implicit}
The equation \(\symbf{r}\left( t \right) = \abracket*{x\left( t
\right),\: y\left( t \right)}\) can be written in explicit or implicit
form, if the parameter \(t\) can be eliminated from the simultaneous
equations.
\subsubsection{Explicit to Parametric}
The equation \(y = f\left( x \right)\) can always be converted to
parametric form by choosing the parameter \(t = x\), so that
\begin{equation*}
    \symbf{r}\left( t \right) = \abracket*{t,\: f\left( t \right)}.
\end{equation*}
\subsubsection{Implicit to Parametric}
The equation \(F\left( x,\: y \right) = 0\) can be converted to
parametric form if we can find \(x = p\left( t \right)\) and \(y =
q\left( t \right)\), such that \(F\left( p\left( t \right),\: q\left( t
\right) \right) = 0\), and
\begin{equation*}
    \symbf{r}\left( t \right) = \abracket*{p\left( t \right),\: q\left( t \right)}
\end{equation*}
for all \(t\).
\subsection{Paramaterisation}
To parametrise a curve, consider the following strategies:
\begin{itemize}
    \item For a closed curve, consider the polar parametrisation in
          terms of the angle \(\theta\):
          \begin{equation*}
              \symbf{r}\left( \theta \right) = \abracket*{R\left( \theta \right) \cos{\left( \theta \right)},\: R\left( \theta \right) \sin{\left( \theta \right)}}.
          \end{equation*}
    \item For a curve that is the intersection of two surfaces,
          consider one of the following mappings:
          \begin{equation*}
              x \mapsto
              \begin{bmatrix}
                  x                 \\
                  y\left( x \right) \\
                  z\left( x \right)
              \end{bmatrix}
              \qquad
              y \mapsto
              \begin{bmatrix}
                  x\left( y \right) \\
                  y                 \\
                  z\left( y \right)
              \end{bmatrix}
              \qquad
              z \mapsto
              \begin{bmatrix}
                  x\left( z \right) \\
                  y\left( z \right) \\
                  z
              \end{bmatrix}
          \end{equation*}
    \item Otherwise, consider a vector construction.
\end{itemize}
\subsubsection{Line Segments}
To parametrise a line segment from point \(A\) to \(B\), define the
parameter \(t \in \interval{0}{1}\). Then, consider the vectors
\(\symbf{a} = \overline{OA}\) and \(\symbf{b} = \overline{OB}\). By
scaling the vector from \(A\) to \(B\) by \(t\), we can parametrise the
line segment as
\begin{equation*}
    \symbf{r}\left( t \right) = \symbf{a} + t \left( \symbf{b} - \symbf{a} \right) = \symbf{a} \left( 1 - t \right) + \symbf{b} t.
\end{equation*}
\subsubsection{Circles}
To parametrise a circle of radius \(R\) centred at the
\(\abracket*{x_0,\: y_0}\), first parametrise the curve in terms of the
angle \(\theta\), then shift the curve by \(\abracket*{x_0,\: y_0}\).
\begin{equation*}
    \symbf{r}\left( \theta \right) =
    \begin{bmatrix}
        x_0 \\
        y_0
    \end{bmatrix}
    +
    \begin{bmatrix}
        R \cos{\left( \theta \right)} \\
        R \sin{\left( \theta \right)}
    \end{bmatrix}
    =
    \begin{bmatrix}
        x_0 + R \cos{\left( \theta \right)} \\
        y_0 + R \sin{\left( \theta \right)}
    \end{bmatrix}
\end{equation*}
\subsubsection{Velocity Vectors}
The velocity vector of a parametrised curve \(\symbf{r}\left( t \right)
=
\begin{bmatrix}
    x\left( t \right) \\
    y\left( t \right)
\end{bmatrix}
\) is defined as
\begin{equation*}
    \symbf{v}\left( t \right) = \symbf{r}'\left( t \right) = \lim_{\Delta t \to 0} \frac{\symbf{r}\left( t + \Delta t \right) - \symbf{r}\left( t \right)}{\Delta t} =
    \begin{bmatrix}
        x'\left( t \right) \\
        y'\left( t \right)
    \end{bmatrix}
\end{equation*}
where \(\symbf{v}\left( t \right)\) is a tangent vector to the curve at the point \(\symbf{r}\left( t \right)\), for all \(t\).
\subsubsection{Tangent Vectors}
Following from the definition of the velocity vector, the tangent
vectors of a parametrised curve are unit vectors in the direction of
the velocity vector.
\begin{equation*}
    \hat{\symbf{\tau}}\left( t \right) = \pm \frac{\symbf{v}\left( t \right)}{\norm{\symbf{v}\left( t \right)}} = \pm \hat{\symbf{v}\left( t \right)}
\end{equation*}
For a curve given in explicit form \(y = f\left( x \right)\), the tangent vectors are given by
\begin{equation*}
    \hat{\symbf{\tau}}\left( x \right) = \pm \frac{1}{\sqrt{1 + \left( f'\left( x \right) \right)^2}}
    \begin{bmatrix}
        1 \\
        f'\left( x \right)
    \end{bmatrix}
\end{equation*}
\section{Multivariable Calculus}
\subsection{Multivariable Functions}
\begin{definition}[Multivariable Function]
    A multivariable function \(f\) maps several independent variables to a real number:
    \begin{equation*}
        f : E \subset \R^n \to \R
    \end{equation*}
    where
    \begin{align*}
        \abracket*{x_1,\: x_2,\: \ldots,\: x_n} & \mapsto z = f\left( x_1,\: x_2,\: \ldots,\: x_n \right) \\
        \symbf{x}                               & \mapsto z = f\left( \symbf{x} \right)
    \end{align*}
\end{definition}
\begin{definition}[Domain]
    The domain of \(f\) is the subset of \(\R^n\) for which \(f\) is defined.
    It corresponds to the set of all possible inputs to \(f\).
    \begin{equation*}
        \mathscr{D} \left( f \right) = E
    \end{equation*}
\end{definition}
\begin{definition}[Range]
    The range of \(f\) is the image of the domain \(E\) under \(f\):
    \begin{equation*}
        \mathscr{R} \left( f \right) = \left\{ f\left( \symbf{x} \right) \in \R : \symbf{x} \in E \right\}
    \end{equation*}
\end{definition}
\begin{definition}[Graph]
    The graph of \(f\) is the defined as the set
    \begin{equation*}
        G = \left\{ \abracket*{\symbf{x},\: f\left( \symbf{x} \right)} : \symbf{x} \in E \right\} \subset \R^{n + 1}
    \end{equation*}
\end{definition}
\subsection{Curves of Intersection}
\begin{definition}[Curves of Intersection]
    The curves of intersection of \(f\) with the plane \(x_i = c\) are defined as
    \begin{equation*}
        \left\{ \abracket*{x_1,\: x_2,\: \ldots,\: x_{i - 1},\: c,\: x_{i + 1},\: \ldots,\: x_n} : \abracket*{x_1,\: x_2,\: \ldots,\: x_n} \in E \right\}
    \end{equation*}
    In 3D, curves of intersection of \(z = f\left( x,\: y \right)\) with the planes perpendicular to the
    \(x\), \(y\), or \(z\)-axes allow us to represent the function in 3D.
    \begin{align*}
        \perp x \: (x = c)                             &  & \perp y \: (y = c)                             &  & \perp z \: (z = c)                                        \\
        z = f\left( c,\: y \right) = g\left( y \right) &  & z = f\left( x,\: c \right) = g\left( x \right) &  & c = f\left( x,\: y \right) \implies y = g\left( x \right)
    \end{align*}
\end{definition}
\begin{definition}[Level Set]
    The level sets of \(f\) are the set of all points in the domain of \(f\) that map to a given value \(c\).
    \begin{equation*}
        \left\{ \symbf{x} \in E : f\left( \symbf{x} \right) = c \right\}
    \end{equation*}
    In 2D, level sets are called \textbf{level curves}.
\end{definition}
\begin{definition}[Contour Map]
    The projection of all level curves onto the \(xy\)-plane is called the \textbf{contour map} of \(f\). T
    he lines of a contour map are called \textbf{contours}.
\end{definition}
\subsection{Derivatives}
\begin{definition}[Continuity]
    A function \(f\) is continuous at a point \(\symbf{x}_0\) if
    \begin{equation*}
        \forall \varepsilon > 0,\: \exists \delta > 0 : \norm{\symbf{x} - \symbf{x}_0} < \delta \implies \norm{f\left( \symbf{x} \right) - f\left( \symbf{x}_0 \right)} < \varepsilon
    \end{equation*}
    Equivalently, \(f\) is continuous at \(\symbf{x}_0\) when
    \begin{equation*}
        \lim_{n \to \infty} f\left( \symbf{x}_n \right) = f\left( \symbf{x}_0 \right)
    \end{equation*}
    for all sequences \(\left\{ \symbf{x}_n \right\} \) where \(\symbf{x}_n \to \symbf{x}_0\).

    A function \(f\) is continuous on a set \(E\) if it is continuous
    at every point in \(E\).
\end{definition}
\begin{definition}[Partial Derivatives]
    Partial derivatives represent the rate of change of a function with respect to one of its variables, holding all other variables constant.
    \begin{equation*}
        \pdv{f}{x_i} = f_{x_i} = \lim_{h \to 0} \frac{f\left( x_1,\: x_2,\: \ldots,\: x_i + h,\: \ldots,\: x_n \right) - f\left( x_1,\: x_2,\: \ldots,\: x_i,\: \ldots,\: x_n \right)}{h}
    \end{equation*}
\end{definition}
\begin{definition}[Higher-order Partial Derivatives]
    Higher-order partial derivatives are defined as
    \begin{equation*}
        \pdv[order=n]{f}{x_i} = \pdv{}{x_i} \left( \pdv[order={n - 1}]{f}{x_i} \right)
    \end{equation*}
\end{definition}
\begin{definition}[Mixed Partial Derivatives]
    Mixed partial derivatives are given the following notation
    \begin{equation*}
        \pdv{f}{x_i,x_j} = \pdv{}{x_i} \left( \pdv{f}{x_j} \right) = f_{x_i x_j}
    \end{equation*}
\end{definition}
\begin{theorem}[Schwarz's Theorem (or Clairaut's Theorem)]
    For a function \(f : E \subset \R^2 \to \R\) with \(\symbf{x}_a \in E\), if \(f_{xy}\) and \(f_{yx}\) are continuous on \(\symbf{x}_0\), then
    \begin{equation*}
        f_{xy} = f_{yx}
    \end{equation*}
\end{theorem}
\begin{remark}
    For an arbitrary \(n\) and \(k \leqslant n\), if all partials of order \(\leqslant \! k\) are continuous in the neighbourhood of \(\symbf{x}_0\),
    then mixed partials of order \(k\) are equal for any permutation of indices:
    \begin{equation*}
        \pdv[mixed-order=k]{f}{x_{i_1}...,x_{i_k}} = \pdv[mixed-order=k]{f}{x_{j_1}\ldots,x_{j_k}}
    \end{equation*}
\end{remark}
\subsection{Chain Rule}
For a function with multiple arguments, each argument of \(f\) that has
an implicit dependence on the variable of differentiation must be
differentiated using the chain rule, where all contributions are
summed.
\begin{gather*}
    \odv{}{t} f\left( x\left( t \right),\: y\left( t \right) \right) = \pdv{f}{x} \odv{x}{t} + \pdv{f}{y} \odv{y}{t} \\
    \pdv{}{u} f\left( x\left( u,\: v \right),\: y\left( u,\: v \right) \right) = \pdv{f}{x} \pdv{x}{u} + \pdv{f}{y} \pdv{y}{u}
\end{gather*}
\begin{definition}[Total derivative]
    The total derivative of a function \(f : \R^n \to \R\) is defined as
    \begin{equation*}
        \odv{f}{t} = \sum_{i = 1}^n \pdv{f}{x_i} \odv{x_i}{t} = \pdv{f}{x_1} \odv{x_1}{t} + \pdv{f}{x_2} \odv{x_2}{t} + \cdots + \pdv{f}{x_n} \odv{x_n}{t}
    \end{equation*}
\end{definition}
\begin{proof}
    Consider the case where \(n = 2\). The total derivative can be written as
    \begin{equation*}
        \odv{}{t} f\left( x\left( t \right),\: y\left( t \right) \right) = \lim_{\Delta{t} \to 0} \frac{1}{\Delta{t}} \left[ f\left( x\left( t + \Delta{t} \right),\: y\left( t + \Delta{t} \right) \right) - f\left( x\left( t \right),\: y\left( t \right) \right) \right]
    \end{equation*}
    Using the substitutions \(x\left( t + \Delta{t} \right) = x\left( t \right) + \Delta{x}\) and \(y\left( t + \Delta{t} \right) = y\left( t \right) + \Delta{y}\),
    \begin{align*}
        \odv{f}{t} & = \lim_{\Delta{t} \to 0} \frac{1}{\Delta{t}} \left[ f\left( x + \Delta{x},\: y + \Delta{y} \right) - f\left( x,\: y \right) \right]                                                                                                                                                   \\
                   & = \lim_{\Delta{t} \to 0} \frac{1}{\Delta{t}} \left[ f\left( x + \Delta{x},\: y + \Delta{y} \right) - f\left( x,\: y + \Delta{y} \right) + f\left( x,\: y + \Delta{y} \right) - f\left( x,\: y \right) \right]                                                                         \\
                   & = \lim_{\Delta{t} \to 0} \left[ \frac{\Delta{x}}{\Delta{t}} \frac{f\left( x + \Delta{x},\: y + \Delta{y} \right) - f\left( x,\: y + \Delta{y} \right)}{\Delta{x}} + \frac{\Delta{y}}{\Delta{t}} \frac{f\left( x,\: y + \Delta{y} \right) - f\left( x,\: y \right)}{\Delta{y}} \right] \\
                   & = \odv{x}{t} \pdv{f}{x} + \odv{y}{t} \pdv{f}{y}
    \end{align*}
\end{proof}
\begin{definition}[Gradient]
    The gradient is an operator that collects all partial derivatives of a function into a vector.
    \begin{equation*}
        \symbf{\nabla} =
        \begin{bmatrix}
            \partial_{x_1} \\
            \partial_{x_2} \\ \vdots \\
            \partial_{x_n}
        \end{bmatrix}
        \implies \symbf{\nabla} f =
        \begin{bmatrix}
            \partial_{x_1} f \\
            \partial_{x_2} f \\ \vdots \\
            \partial_{x_n} f
        \end{bmatrix}
    \end{equation*}
\end{definition}
The gradient allows us to define the chain rule using the dot product:
\begin{equation*}
    \odv{f}{t} = \symbf{\nabla} f \cdot \odv{\symbf{x}}{t}
\end{equation*}
\subsection{Directional Derivative}
The directional derivative of a function \(f : E \subset \R^n \to \R\)
at a point \(\symbf{x}_0 \in E\) in the direction of a unit vector
\(\hat{\symbf{u}} \in \R^n\) is the slope of \(f\) in the direction of
\(\symbf{u}\):
\begin{equation*}
    D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \partial_{\symbf{u}} f\left( \symbf{x}_0 \right) = \lim_{h \to 0}
     \frac{f\left( \symbf{x}_0 + h \hat{\symbf{u}} \right) - f\left(
     \symbf{x}_0 \right)}{h}
\end{equation*}
\begin{proposition}
    The directional derivative can be computed using the gradient of \(f\)
    \begin{equation*}
        D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \symbf{\nabla} f\left( \symbf{x}_0 \right) \cdot \hat{\symbf{u}}.
    \end{equation*}
\end{proposition}
\begin{proof}
    Consider the path in the output space \(g\left( s \right) = f\left( \symbf{x}_0 + s \hat{\symbf{u}} \right) \in \R\) for \(s \in \R\).
    The derivative of \(g\) w.r.t.\ \(s\) is given by:
    \begin{equation*}
        \odv{g}{s} = \lim_{h \to 0} \frac{g\left( s + h \right) - g\left( s \right)}{h}
    \end{equation*}
    where if we evaluate \(g\) at \(s = 0\), we get
    \begin{equation*}
        \odv{g}{s}_{s = 0} = \lim_{h \to 0} \frac{g\left( h \right) - g\left( 0 \right)}{h} = \lim_{h \to 0} \frac{f\left( \symbf{x}_0 + h \hat{\symbf{u}} \right) - f\left( \symbf{x}_0 \right)}{h} = D_{\symbf{u}} f\left( \symbf{x}_0 \right).
    \end{equation*}
    Using the derivative of the parametrised vector \(\symbf{x} = \symbf{x}_0 + s \hat{\symbf{u}} \in \R^n\):
    \begin{equation*}
        \odv{\symbf{x}}{s} = \hat{\symbf{u}}
    \end{equation*}
    we can evaluate the derivative of \(g\) w.r.t.\ \(s\) using the total derivative:
    \begin{align*}
        \odv{g}{s}_{s = 0} & = \sum_{i = 1}^n \pdv{f}{x_i}_{s = 0} \odv{x_i}{s}_{s = 0}               \\
                           & = \sum_{i = 1}^n \pdv{f}{x_i}_{s = 0} \hat{u}_i                          \\
                           & = \symbf{\nabla} f\left( \symbf{x} \right)_{s = 0} \cdot \hat{\symbf{u}} \\
                           & = \symbf{\nabla} f\left( \symbf{x}_0 \right) \cdot \hat{\symbf{u}}
    \end{align*}
    where \(\hat{u}_i\) is the \(i\)-th component of \(\hat{\symbf{u}}\).
    Therefore \(D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \symbf{\nabla} f\left( \symbf{x}_0 \right) \cdot \hat{\symbf{u}}\).
\end{proof}
\begin{remark}
    Partial derivatives are directional derivatives in the direction of the canonical basis vectors \(\hat{\symbf{e}}_i\).
    \begin{equation*}
        \pdv{f}{x_i} = D_{\hat{\symbf{e}}_i} f
    \end{equation*}
    We can therefore say that the directional derivative is a generalisation of the partial derivative
    for any direction \(\hat{\symbf{u}}\).
\end{remark}
\begin{proposition}
    The gradient of a function \(\symbf{\nabla} f\) is orthogonal to the level curves of \(f\).
\end{proposition}
\begin{proof}
    Consider the path \(\symbf{x}\left( s \right)\) on a contour of \(f\). As \(f\) is constant on the contour,
    \begin{equation*}
        \pdv{f}{s} = 0.
    \end{equation*}
    Using the chain rule,
    \begin{equation*}
        \pdv{f}{s} = \sum_{i = 1}^n \pdv{f}{x_i} \pdv{x_i}{s} = \symbf{\nabla} f \cdot \odv{\symbf{x}}{s} = 0
    \end{equation*}
    therefore as the dot product is zero, \(\symbf{\nabla} f\) is orthogonal to the path \(\symbf{x}\left( s \right)\).
\end{proof}
\begin{proposition}
    The directional derivative is maximised when \(\hat{\symbf{u}}\) is parallel to \(\symbf{\nabla} f\).
\end{proposition}
\begin{proof}
    Using the angle definition of the dot product, the directional derivative is given by
    \begin{equation*}
        D_{\symbf{u}} f = \symbf{\nabla} f \cdot \hat{\symbf{u}} = \norm{\symbf{\nabla} f} \norm{\hat{\symbf{u}}} \cos{\theta} = \norm{\symbf{\nabla} f} \cos{\theta}
    \end{equation*}
    This expression is maximised when \(\cos{\theta} = 1\), or when \(\symbf{u}\) is parallel to \(\symbf{\nabla} f\).
\end{proof}
\begin{remark}
    The maximum slope of \(f\) at \(\symbf{x}_0\) is given by the magnitude of the gradient at \(\symbf{x}_0\):
    \begin{equation*}
        \max_{\hat{\symbf{u}}} D_{\symbf{u}} f\left( \symbf{x}_0 \right) = \norm{\symbf{\nabla} f\left( \symbf{x}_0 \right)}
    \end{equation*}
\end{remark}
\subsection{Normal Vectors to Curves}
\subsubsection{Parametric Curves}
For a parametric curve \(\symbf{r}\left( t \right)\), we find a normal
vector \(\hat{\symbf{n}}\) such that
\begin{equation*}
    \hat{\symbf{n}} \cdot \symbf{r}'\left( t \right) = 0
\end{equation*}
where \(\symbf{r}'\left( t \right)\) is a tangent vector to the curve.
\subsubsection{Implicit Curves}
For an implicit curve \(F\left( x,\: y \right) = 0\), the normal vector
is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{\nabla} F}{\norm{\symbf{\nabla} F}}
\end{equation*}
\subsubsection{Explicit Curves}
For an explicit curve \(y = f\left( x \right)\), we must convert the
curve to implicit or parametric form.
\subsection{Normal Vectors to Surfaces}
\subsubsection{Parametric Surfaces}
\subsubsection{Implicit Surfaces}
For an implicit surface \(F\left( x,\: y,\: z \right) = 0\), the normal
vector is given by
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{\nabla} F}{\norm{\symbf{\nabla} F}}
\end{equation*}
\subsubsection{Explicit Surfaces}
For an explicit surface \(z = f\left( x,\: y \right)\), we can either
convert the surface to implicit form, or consider the tangents of two
curves \(\symbf{r}_1\left( t \right)\) and \(\symbf{r}_2\left( t
\right)\) on the surface. Then we can find the normal vector by taking
the cross product of the tangent vectors:
\begin{equation*}
    \hat{\symbf{n}} = \pm \frac{\symbf{r}_1'\left( t \right) \times \symbf{r}_2'\left( t \right)}{\norm{\symbf{r}_1'\left( t \right) \times \symbf{r}_2'\left( t \right)}}
\end{equation*}
\subsection{Tangent Vectors to Curves}
\subsubsection{Parametric Curves}
For a parametric curve \(\symbf{r}\left( t \right)\), the tangent
vector is given by
\begin{equation*}
    \hat{\symbf{\tau}} = \pm \frac{\symbf{r}'\left( t \right)}{\norm{\symbf{r}'\left( t \right)}}
\end{equation*}
\subsubsection{Implicit Curves in 2D}
For an implicit curve \(F\left( x,\: y \right) = 0\), the tangent
vector can be found by first determining the normal vector
\(\hat{\symbf{n}}\) which is proportional to the gradient of \(F\):
\begin{equation*}
    \hat{\symbf{n}} =
    \begin{bmatrix}
        n_1 \\
        n_2
    \end{bmatrix}
    \propto \symbf{\nabla} f
\end{equation*}
such that the tangent vector is given by
\begin{equation*}
    \hat{\symbf{\tau}} = \pm
    \begin{bmatrix}
        -n_2 \\
        n_1
    \end{bmatrix}
\end{equation*}
\subsubsection{Implicit Curves in 3D}
Given the intersectin of two implicit curves \(F\left( x,\: y,\: z
\right) = 0\) and \(G\left( x,\: y,\: z \right) = 0\), the tangent
vector along the intersection is given by
\begin{equation*}
    \hat{\symbf{\tau}} = \pm \frac{\symbf{\nabla} F \times \symbf{\nabla} G}{\norm{\symbf{\nabla} F \times \symbf{\nabla} G}}
\end{equation*}
\subsubsection{Explicit Curves in 2D}
For an explicit curve \(y = f\left( x \right)\), we can either:
\begin{itemize}
    \item convert the curve to parametric form: \(\symbf{r}\left( t
          \right) = \abracket*{t,\: f\left( t \right)}\)
    \item convert the curve to implicit form: \(F\left( x,\: y \right)
          = y - f\left( x \right) = 0\)
\end{itemize}
\subsection{Differentiability}
\subsection{Single Variable Functions}
\(f\left( x \right)\) is differentiable at \(a\) if \(f'\left( a \right) = \lim_{x \to a} \frac{f\left( x \right) - f\left( a \right)}{x - a}\) exists.
This is equivalent to saying that the tangent line at \(\abracket*{a,\: f\left( a \right)}\) is well-defined:
\begin{equation*}
    f\left( x \right) = \underbrace{f\left( a \right) + f'\left( a \right) f\left( x - a \right)}_{\text{tangent line}} + R\left( x - a \right) \qquad \text{with } \lim_{x \to a} \frac{R\left( x - a \right)}{x - a} = 0
\end{equation*}
\subsection{Multivariable Functions}
\(f\left( \symbf{x} \right)\) is differentiable at \(\symbf{a} \in \Omega \subset \R^n\) if there exists a linear map \(f'\left( \symbf{a} \right) :
\begin{cases*}
    \R^n \to \R                                      \\
    \xi \mapsto f'\left( \symbf{a} \right) \cdot \xi \\
\end{cases*}
\) such that
\begin{equation*}
    f\left( \symbf{x} \right) = \underbrace{f\left( \symbf{a} \right) + f'\left( \symbf{a} \right) \cdot \left( \symbf{x} - \symbf{a} \right)}_{\text{tangent plane}} + R\left( \symbf{x} - \symbf{a} \right) \qquad \text{with } \lim_{\symbf{x} \to \symbf{a}} \frac{R\left( \symbf{x} - \symbf{a} \right)}{\norm{\symbf{x} - \symbf{a}}} = 0
\end{equation*}
The linear map \(f'\left( \symbf{a} \right)\) is then the derivative of \(f\) at \(\symbf{a}\).
\begin{theorem}[Derivative Equivalence with Gradient]
    If \(f\) is differentiable, then
    \begin{equation*}
        f'\left( \symbf{a} \right) = \symbf{\nabla} f\left( \symbf{a} \right)
    \end{equation*}
    Additionally, if \(\symbf{\nabla} f\left( \symbf{a} \right)\) exists
    \(\forall \symbf{a} \in \Omega\), and all partial derivatives are
    continuous (i.e., \(\pdv{f}{x_i} : \Omega \subset \R^n \to R\) are
    continuous), then \(f\) is differentiable everywhere in \(\Omega\).
\end{theorem}
The first result tells us that the derivative is unique, and that \(f\) is
differentiable at \(\symbf{a}\) if it has a tangent plane at \(\symbf{a}\).
The second result gives us a sufficient condition for differentiability.
\subsection{Taylor Series Expansion}
The Taylor series expansion of a function \(f\) at a point
\(\symbf{a}\) is given by
\begin{multline*}
    f\left( \symbf{x} \right) = f\left( \symbf{a} \right) + \sum_{i = 0}^n \pdv{f\left( \symbf{a} \right)}{x_i} \left( x_i - a_i \right) + \frac{1}{2} \sum_{i = 1}^n \sum_{j = 1}^n \pdv{f\left( \symbf{a} \right)}{x_i,x_j} \left( x_i - a_i \right) \left( x_j - a_j \right) + \cdots \\
    + \frac{1}{k!} \sum_{i_1, \ldots, i_k}^{n} \pdv{f\left( \symbf{a} \right)}{x_{i_1}, \cdots, x_{i_k}} \left( x_{i_1} - a_{i_1} \right) \cdots \left( x_{i_k} - a_{i_k} \right) + \cdots
\end{multline*}
This allows us to compute the tangent plane, paraboloid, and so on, of
a function \(f\) at \(\symbf{a}\), by increasing the order \(k\) of the
Taylor series expansion.
\section{Double Integrals}
Integrals represent continuous sums of infinitesimal quantities, and
allow us to measure extensive and average properties of objects, such
as lengths, surface areas, volumes, masses, centres of mass, and so on.
\subsection{Riemann Sums}
The signed area under the function \(f\left( x \right)\) is represented
by
\begin{equation*}
    \int_a^b f\left( x \right) \odif{x} = \lim_{n \to \infty} \sum_{i = 1}^n f\left( x_i \right) \Delta x
\end{equation*}
Similarly, the signed volume under the surface \(f\left( x,\: y \right)\)
is represented by
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} = \lim_{\Delta A_i \to 0} \sum_{i = 1}^n f\left( x_i,\: y_i \right) \Delta A_i
\end{equation*}
where \(R\) is the region of integration.
\subsection{Lebesgue Integrals}
Lebesgue integrals are a top down approach to multiple integrals that
allow us to integrate a wider class of nonnegative functions. This
includes improper integrals where \(f\) is discontinuous or singular,
or when the domain of integration is unbounded.
\begin{theorem}[Fubini's Theorem]
    For a nonnegative function \(f : \R^2 \to \rinterval{0}{\infty}\),
    the following equality holds,
    \begin{equation*}
        0 \leqslant \iint_{\R^2} f\left( x,\: y \right) \odif{A} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{x} \right] \odif{y} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{y} \right] \odif{x} \leqslant \infty
    \end{equation*}
\end{theorem}
The above theorem holds for particular classes of functions. The
following techniques can be used to integrate arbitrary functions.
\subsubsection{Finite Domains}
For double integrals over domains \(R \subset \R^2\), we can define a
new function \(\tilde{f}\) such that
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} \equiv \iint_{\R^2} \tilde{f}\left( x,\: y \right) \odif{A}, \qquad \text{where } \tilde{f}\left( x,\: y \right) =
    \begin{cases}
        f\left( x,\: y \right) & \left( x,\: y \right) \in R \\
        0                      & \text{otherwise}
    \end{cases}
\end{equation*}
\subsubsection{Nonpositive Functions}
For nonpositive functions \(f : \R^2 \to \ointerval{-\infty}{\infty}\),
we can express \(f\) as \(f = f^{+} - f^{-}\), where \(f^{+}\) and
\(f^{-}\) are the positive and negative parts of \(f\) respectively:
\begin{align*}
    f^{+}\left( x,\: y \right) & = \max{\left\{ 0,\: f\left( x,\: y \right) \right\}}
    =
    \begin{cases}
        f\left( x,\: y \right) & f\left( x,\: y \right) \geqslant 0 \\
        0                      & f\left( x,\: y \right) < 0
    \end{cases}
    \\
    f^{-}\left( x,\: y \right) & = -\min{\left\{ 0,\: f\left( x,\: y \right) \right\}}
    =
    \begin{cases}
        -f\left( x,\: y \right) & f\left( x,\: y \right) < 0         \\
        0                       & f\left( x,\: y \right) \geqslant 0
    \end{cases}
\end{align*}
Therefore, the double integral is defined
\begin{equation*}
    \iint_{\R^2} f\left( x,\: y \right) \odif{A} = \iint_{\R^2} f^{+}\left( x,\: y \right) \odif{A} - \iint_{\R^2} f^{-}\left( x,\: y \right) \odif{A}
\end{equation*}
\begin{definition}[Integrable Function]
    A function \(f : \R^n \to \R\) is \textbf{integrable} if
    \begin{equation*}
        \int_{\R^n} \abs{f\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty
    \end{equation*}
\end{definition}
\begin{theorem}[Fubini's Theorem for Integrable Functions]
    If \(f : \R^2 \to \R\) is an integrable function
    \begin{equation*}
        \iint_{\R^2} f\left( x,\: y \right) \odif{A} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{x} \right] \odif{y} = \int_{-\infty}^\infty \left[ \int_{-\infty}^\infty f\left( x,\: y \right) \odif{y} \right] \odif{x}
    \end{equation*}
\end{theorem}
\subsection{Measure of a Region}
\begin{definition}[Indicator Function]
    The indicator function of \(R\) is defined:
    \begin{equation*}
        1_R\left( x,\: y \right) =
        \begin{cases}
            1 & \left( x,\: y \right) \in R \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{definition}
\begin{definition}[Measure]
    The measure of a region \(R\) is defined as
    \begin{equation*}
        \mu\left( R \right) = \iint_{\R^2} 1_R \odif{A}
    \end{equation*}
    In two dimensions, the measure is the area of the region.
\end{definition}
\subsubsection{Rectangular Regions}
For a rectangular region \(R = \left[ a,\: b \right] \times \left[ c,\:
d \right]\), the indicator function is given by
\begin{align*}
    1_R\left( x,\: y \right) & =
    \begin{cases}
        1 & \left( x,\: y \right) \in D \\
        0 & \text{otherwise}
    \end{cases}
    \\
                             & = 1_{\left[ a,\: b \right]} \left( x \right) 1_{\left[ c,\: d \right]} \left( y \right)
\end{align*}
so that the area of \(R\) is given by
\begin{align*}
    \mu\left( R \right) & = \iint_{\R^2} 1_R \odif{A} = \int_{-\infty}^\infty \int_{-\infty}^\infty 1_{\left[ a,\: b \right]} \left( x \right) 1_{\left[ c,\: d \right]} \left( y \right) \odif{y} \odif{x} = \int_{-\infty}^\infty 1_{\left[ a,\: b \right]} \left( x \right) \int_{-\infty}^\infty 1_{\left[ c,\: d \right]} \left( y \right) \odif{y} \odif{x} \\
                        & = \int_a^b 1 \left[ \int_c^d 1 \odif{y} \right] \odif{x} = \int_a^b \left( d - c \right) \odif{x} = \left( b - a \right) \left( d - c \right)
\end{align*}
\subsubsection{Integrating Functions over Regions}
When integrating over a nonuniform density \(f\), we can multiply \(f\)
with the indicator function over \(R\):
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} = \iint_{\R^2} f\left( x,\: y \right) 1_R \odif{A}
\end{equation*}
\subsection{Simple Domains}
\subsubsection{Type I --- \texorpdfstring{\(y\)}{y}-Simple}
Let \(Y\) be defined as a region bounded by
\(x_1 \leqslant x \leqslant x_2\) and
\(y_1\left( x \right) \leqslant y \leqslant y_2\left( x \right)\).
\(Y\) is called \(y\)-simple as it can be split into subdomains \(R_i\)
with continuous lines \textbf{parallel to the \(y\)-axis}.

The double integral over \(Y\) can then be decomposed into the sum of
integrals over each subdomain \(Y_i\):
\begin{equation*}
    \iint_Y f\left( x,\: y \right) \odif{A} = \sum_{i = 1}^n \iint_{Y_i} f\left( x,\: y \right) \odif{A}
\end{equation*}
which is equivalent to integrating between the \(y\)-bounds of each
subdomain \(Y_i\) within the \(x\)-bounds of \(Y\):
\begin{equation*}
    \iint_Y f\left( x,\: y \right) \odif{A} = \int_{x_1}^{x_2} \left[ \int_{y_1\left( x \right)}^{y_2\left( x \right)} f\left( x,\: y \right) \odif{y} \right] \odif{x}
\end{equation*}
\subsubsection{Type II --- \texorpdfstring{\(x\)}{x}-Simple}
Let \(X\) be defined as a region bounded by
\(x_1\left( y \right) \leqslant x \leqslant x_2\left( y \right)\) and
\(y_1 \leqslant y \leqslant y_2\).
\(X\) is called \(x\)-simple as it can be split into subdomains \(R_i\)
with continuous lines \textbf{parallel to the \(x\)-axis}.

The double integral over \(X\) can then be decomposed into the sum of
integrals over each subdomain \(X_i\):
\begin{equation*}
    \iint_X f\left( x,\: y \right) \odif{A} = \sum_{i = 1}^n \iint_{X_i} f\left( x,\: y \right) \odif{A}
\end{equation*}
which is equivalent to integrating between the \(x\)-bounds of each
subdomain \(X_i\) within the \(y\)-bounds of \(X\):
\begin{equation*}
    \iint_X f\left( x,\: y \right) \odif{A} = \int_{y_1}^{y_2} \left[ \int_{x_1\left( y \right)}^{x_2\left( y \right)} f\left( x,\: y \right) \odif{x} \right] \odif{y}
\end{equation*}
\subsection{Transformation of Coordinates}
In single variable calculus, we can use the transformation \(x =
g\left( u \right)\) to change the variable of integration from \(x\) to
\(u\), using the chain rule, \(\odif{x} = g'\left( u \right)
\odif{u}\). For multiple integrals, the same can be accomplished using
the Jacobian matrix.
\begin{definition}[Jacobian Matrix]
    Consider the transformation \(\symbf{x} = T\left( \symbf{u} \right)\),
    where \(T : \R^m \to \R^n\) is once differentiable. The Jacobian
    matrix of \(T\) is defined as an \(m \times n\) matrix, denoted
    \(\symbf{J}\), whose \(\left( i,\: j \right)\)-th entry is given by
    \begin{equation*}
        \symbf{J}_{i,\: j} = \pdv{x_i}{u_j}.
    \end{equation*}
    Explicitly, the Jacobian matrix is given by
    \begin{equation*}
        \symbf{J} =
        \displaystyle
        \begin{bmatrix}
            \displaystyle\pdv{x_1}{u_1} & \displaystyle\cdots & \displaystyle\pdv{x_1}{u_n} \\
            \displaystyle\vdots         & \displaystyle\ddots & \displaystyle\vdots         \\
            \displaystyle\pdv{x_m}{u_1} & \displaystyle\cdots & \displaystyle\pdv{x_m}{u_n} \\
        \end{bmatrix}
        .
    \end{equation*}
    It may also be notated as
    \begin{equation*}
        \symbf{J} = \frac{\partial\left( x_1,\: \ldots,\: x_m \right)}{\partial\left( u_1,\: \ldots,\: u_n \right)}
    \end{equation*}
\end{definition}
\begin{definition}[Jacobian]
    When \(m = n\), we can determine the ratio of the area of the
    original region \(R\) to the area of the transformed region
    \(T\left( R \right)\) using the determinant of the Jacobian matrix:
    \begin{equation*}
        \abs*{\symbf{J}} = \det{\left( \symbf{J} \right)} = \abs*{\frac{\partial\left( x_1,\: \ldots,\: x_n \right)}{\partial\left( u_1,\: \ldots,\: u_n \right)}}.
    \end{equation*}
\end{definition}
In the case of two variables, \(\left( x,\: y \right) = T\left( u,\: v \right)\),
and the Jacobian matrix is defined as
\begin{equation*}
    \symbf{J} =
    \begin{bmatrix}
        \displaystyle\pdv{x}{u} & \displaystyle\pdv{x}{v} \\
        \displaystyle\pdv{y}{u} & \displaystyle\pdv{y}{v} \\
    \end{bmatrix}
    .
\end{equation*}
The Jacobian is then given by
\begin{equation*}
    \abs*{\symbf{J}} = \pdv{x}{u} \pdv{y}{v} - \pdv{x}{v} \pdv{y}{u}
\end{equation*}
\subsubsection{General Transformation}
Using this matrix determinant allows us to change the variables of
integration from \(\left( x,\: y \right)\) to \(\left( u,\: v \right)\)
using the following formula:
\begin{equation*}
    \iint_{R} f\left( x,\: y \right) \odif{x}\odif{y} = \iint_{T\left( R \right)} f\left( T\left( u,\: v \right) \right) \abs*{\det{\left( \symbf{J} \right)}} \odif{u}\odif{v}
\end{equation*}
where \(f\left( T\left( u,\: v \right) \right)\) is multiplied by the absolute value of the
Jacobian.
\subsubsection{Polar Coordinate Transformation}
\begin{definition}[Polar Coordinates]
    A polar coordinate system is defined by the transformation
    \begin{align*}
        x & = r \cos{\theta} \\
        y & = r \sin{\theta}
    \end{align*}
    where \(r > 0\) is the radius of the circle and \(\theta \in \rinterval{0}{2\pi}\)
    is the angle of rotation measured anticlockwise from the positive
    \(x\)-axis.
    The Jacobian is given by
    \begin{align*}
        \symbf{J}                      & =
        \begin{bmatrix}
            \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\theta} \\
            \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\theta} \\
        \end{bmatrix}
        \\
                                       & =
        \begin{bmatrix}
            \cos{\theta} & -r \sin{\theta} \\
            \sin{\theta} & r \cos{\theta}  \\
        \end{bmatrix}
        \\
        \det{\left( \symbf{J} \right)} & = r \cos^2{\theta} + r \sin^2{\theta} \\
                                       & = r
    \end{align*}
    The infinitesimal area element \(\odif{A}\) is therefore
    \begin{equation*}
        \odif{A} = \abs*{\det{\left( \symbf{J} \right)}} \odif{r}\odif{\theta} = r \odif{r}\odif{\theta}.
    \end{equation*}
\end{definition}
Using polar coordinates, the double integral over a region \(R\) is
defined
\begin{equation*}
    \iint_R f\left( x,\: y \right) \odif{A} = \int_0^\infty \int_0^{2\pi} f\left( r \cos{\theta},\: r \sin{\theta} \right) r \odif{\theta} \odif{r}
\end{equation*}
\subsection{Strategies for Evaluating Double Integrals}
\begin{enumerate}
    \item Decompose the region of integration into simple domains, and
          if this is not possible, divide the region into subregions.
    \item Change the variables of integration to simplify the integral.
    \item Swap the order of integration to simplify the integral.
\end{enumerate}
\section{Multiple Integrals}
All results and properties for double integrals can be extended to
triple integrals.
\begin{theorem}[Fubini's Theorem in \(n\)-dimensions]
    If \(f : \R^n \to \R^+\), or \(f : \R^n \to \R\) is integrable, then
    all \(n!\) permutations of integrals are equal:
    \begin{equation*}
        \int_{\R^n} f\left( x_1,\: \ldots,\: x_n \right) \odif{x_1} \cdots \odif{x_n} = \int_{-\infty}^\infty \left[ \cdots \left[ \int_{-\infty}^\infty f\left( x_1,\: \ldots,\: x_n \right) \odif{x_1} \right] \cdots \right] \odif{x_n}
    \end{equation*}
\end{theorem}
As with double integrals, we can evaluate integrals of nonpositive
functions by using the positive and negative parts of the function.
\subsection{Vector-Valued Functions}
\begin{definition}[Vector-Valued Function]
    A vector-valued function \(\symbf{f} : \R^n \to \R^m\) is a
    function that maps a vector \(\symbf{x} \in \R^n\) to a vector
    \(\symbf{f}\left( \symbf{x} \right) \in \R^m\).
\end{definition}
A vector-valued function is integrable if either:
\begin{itemize}
    \item the norm of the function is integrable
          \begin{equation*}
              \int_{\R^n} \norm{\symbf{f}\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty
          \end{equation*}
    \item or if all components of the function are integrable
          \begin{equation*}
              \int_{\R^n} \abs*{f_i\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty \qquad \forall i \in \left\{ 1,\: \ldots,\: m \right\}
          \end{equation*}
\end{itemize}
The integral of a vector-valued function is defined as
\begin{equation*}
    \int_{\R^n} \symbf{f}\left( \symbf{x} \right) \odif{\symbf{x}} = \abracket*{\int_{\R^n} f_1\left( \symbf{x} \right) \odif{\symbf{x}},\: \ldots,\: \int_{\R^n} f_m\left( \symbf{x} \right) \odif{\symbf{x}}}
\end{equation*}
\subsection{Change of Variables}
\subsubsection{Cylindrical Coordinate Transformation}
\begin{definition}[Cylindrical Coordinates]
    A cylindrical coordinate system is defined by the \linebreak transformation
    \begin{align*}
        x & = r \cos{\theta} \\
        y & = r \sin{\theta} \\
        z & = z
    \end{align*}
    where \(r > 0\) is the radius of the cylinder,
    \(\theta \in \rinterval{0}{2\pi}\) is the angle of rotation
    measured anticlockwise from the positive \(x\)-axis, and \(z \in \R\)
    is the height of the cylinder.
    The Jacobian is given by
    \begin{align*}
        \symbf{J}                      & =
        \begin{bmatrix}
            \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\theta} & \displaystyle\pdv{x}{z} \\
            \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\theta} & \displaystyle\pdv{y}{z} \\
            \displaystyle\pdv{z}{r} & \displaystyle\pdv{z}{\theta} & \displaystyle\pdv{z}{z} \\
        \end{bmatrix}
        \\
                                       & =
        \begin{bmatrix}
            \cos{\theta} & -r \sin{\theta} & 0 \\
            \sin{\theta} & r \cos{\theta}  & 0 \\
            0            & 0               & 1 \\
        \end{bmatrix}
        \\
        \det{\left( \symbf{J} \right)} & = r \cos^2{\theta} + r \sin^2{\theta} \\
                                       & = r
    \end{align*}
    The infinitesimal area element \(\odif{V}\) is therefore
    \begin{equation*}
        \odif{V} = \abs*{\det{\left( \symbf{J} \right)}} \odif{r}\odif{\theta}\odif{z} = r \odif{r}\odif{\theta}\odif{z}.
    \end{equation*}
\end{definition}
Using cylindrical coordinates, the triple integral over a region \(R\) is
defined
\begin{equation*}
    \iint_R f\left( x,\: y,\: z \right) \odif{V} = \int_{-\infty}^{\infty} \int_0^\infty \int_0^{2\pi} f\left( r \cos{\theta},\: r \sin{\theta},\: z \right) r \odif{\theta} \odif{r} \odif{z}
\end{equation*}
\subsubsection{Spherical Coordinate Transformation}
\begin{definition}[Spherical Coordinates]
    A spherical coordinate system is defined by the \linebreak transformation
    \begin{align*}
        x & = \rho \sin{\phi} \cos{\theta} \\
        y & = \rho \sin{\phi} \sin{\theta} \\
        z & = \rho \cos{\phi}
    \end{align*}
    where \(\rho > 0\) is the radius of the sphere,
    \(\phi \in \rinterval{0}{\pi}\) is the polar angle
    measured down from the positive \(z\)-axis,
    and \(\theta \in \rinterval{0}{2\pi}\) is the azimuthal angle measured
    anticlockwise from the positive \(x\)-axis.
    The Jacobian is given by
    \begin{align*}
        \symbf{J}                      & =
        \begin{bmatrix}
            \displaystyle\pdv{x}{\rho} & \displaystyle\pdv{x}{\phi} & \displaystyle\pdv{x}{\theta} \\
            \displaystyle\pdv{y}{\rho} & \displaystyle\pdv{y}{\phi} & \displaystyle\pdv{y}{\theta} \\
            \displaystyle\pdv{z}{\rho} & \displaystyle\pdv{z}{\phi} & \displaystyle\pdv{z}{\theta} \\
        \end{bmatrix}
        \\
                                       & =
        \begin{bmatrix}
            \sin{\phi} \cos{\theta} & \rho \cos{\phi} \cos{\theta} & -\rho \sin{\phi} \sin{\theta} \\
            \sin{\phi} \sin{\theta} & \rho \cos{\phi} \sin{\theta} & \rho \sin{\phi} \cos{\theta}  \\
            \cos{\phi}              & -\rho \sin{\phi}             & 0                             \\
        \end{bmatrix}
        \\
        \det{\left( \symbf{J} \right)} & = \rho^2 \sin^3{\phi} \cos^2{\theta} + \rho^2 \sin{\phi} \cos^2{\phi} \cos^2{\theta} - \rho \sin{\phi} \sin{\theta} \left( -\rho \sin^2{\phi} \sin{\theta} - \rho \cos^2{\phi} \sin{\theta}  \right) \\
                                       & = \rho^2 \sin{\phi} \cos^2{\theta} \left( \sin^2{\phi} + \cos^2{\phi} \right) + \rho^2 \sin{\phi} \sin^2{\theta} \left( \sin^2{\phi} + \cos^2{\phi} \right)                                          \\
                                       & = \rho^2 \sin{\phi} \left( \sin^2{\theta} + \cos^2{\theta} \right)                                                                                                                                   \\
                                       & = \rho^2 \sin{\phi}
    \end{align*}
    The infinitesimal area element \(\odif{V}\) is therefore
    \begin{equation*}
        \odif{V} = \abs*{\det{\left( \symbf{J} \right)}} \odif{\rho}\odif{\phi}\odif{\theta} = \rho^2 \sin{\phi} \odif{\rho}\odif{\phi}\odif{\theta}.
    \end{equation*}
\end{definition}
Using spherical coordinates, the triple integral over a region \(R\) is
defined
\begin{equation*}
    \iiint_R f\left( x,\: y,\: z \right) \odif{V} = \int_0^\infty \int_0^{2\pi} \int_0^\pi f\left( \rho \sin{\phi} \cos{\theta},\: \rho \sin{\phi} \sin{\theta},\: \rho \cos{\phi} \right) \rho^2 \sin{\phi} \odif{\phi} \odif{\theta} \odif{\rho}
\end{equation*}
\subsection{Interpretations of Integrals}
\subsubsection*{Measure}
The measure of a region \(R \in \R^n\) is given by:
\begin{equation*}
    \mu\left( R \right) = \int_R \odif{\symbf{x}}
\end{equation*}
In two dimensions, the measure is the area of the region:
\begin{equation*}
    \mu\left( R \right) = \iint_R \odif{A}
\end{equation*}
In three dimensions, the measure is the volume of the region:
\begin{equation*}
    \mu\left( R \right) = \iiint_R \odif{V}
\end{equation*}
\subsubsection*{Mass}
The mass of a region \(R \in \R^n\) with density function \(\rho\) is
given by:
\begin{equation*}
    M = \int_R \rho\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Centroid}
The average position of a region \(R \in \R^n\) with uniform density is
given by:
\begin{equation*}
    \abracket*{\symbf{r}} = \frac{1}{\mu\left( R \right)} \int_R \symbf{x} \odif{\symbf{x}}
\end{equation*}
This point is the geometric centre of the region where the region would
balance if it were made of a uniform material.
\subsubsection*{Centre of Mass}
The average position of a region \(R \in \R^n\) with density function
\(\rho\) is given by:
\begin{equation*}
    \abracket*{\symbf{r}}_\rho = \frac{1}{M} \int_R \rho\left( \symbf{x} \right) \symbf{x} \odif{\symbf{x}}
\end{equation*}
This point is the centre of mass of the region where the region would
balance if it were made of a material with density \(\rho\).
\subsubsection*{Average Value}
The average value of a function \(f : \R^n \to \R\) over a region \(R
\in \R^n\) is given by:
\begin{equation*}
    \abracket*{f\left( \symbf{r} \right)} = \frac{1}{\mu\left( R \right)} \int_R f\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Expected Value}
The average value of a function \(f : \R^n \to \R\) over a region \(R
\in \R^n\) with density function \(p\) is given by:
\begin{equation*}
    \abracket*{f\left( \symbf{r} \right)}_p = \int_R p\left( \symbf{x} \right) f\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
where \(p\) is a probability density function satisfying
\begin{equation*}
    \int_R p\left( \symbf{x} \right) \odif{\symbf{x}} = 1
\end{equation*}
This result is the expected value of \(f\) over \(R\) if the region
is randomly sampled according to the probability density function \(p\).
\subsection{Properties of Integrals}
\subsubsection*{Linearity of Integrals}
If \(f\) and \(g\) are integrable functions, and \(c \in \R\), then
\begin{equation*}
    \int_{\R^n} \left( f\left( \symbf{x} \right) + g\left( \symbf{x} \right) \right) \odif{\symbf{x}} = \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} + \int_{\R^n} g\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
and
\begin{equation*}
    \int_{\R^n} c f\left( \symbf{x} \right) \odif{\symbf{x}} = c \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Positivity of Integrals}
If \(f \geqslant 0\) for all \(\symbf{x} \in \R^n\), then
\begin{equation*}
    \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} \geqslant 0
\end{equation*}
\subsubsection*{Monotonicity of Integrals}
If \(f\) and \(g\) are integrable functions, and \(f\left( \symbf{x}
\right) \leqslant g\left( \symbf{x} \right)\) for all \(\symbf{x} \in
\R^n\), then
\begin{equation*}
    \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} \leqslant \int_{\R^n} g\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Triangle Inequality}
If \(f\) is an integrable function, then
\begin{equation*}
    \norm*{\int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}}} \leqslant \int_{\R^n} \norm*{f\left( \symbf{x} \right)} \odif{\symbf{x}}
\end{equation*}
\subsubsection*{Change of Variables}
When \(f\) is positive or integrable, the bijective transformation
\(\symbf{T} : R \subset \R^n \to R' \subset \R^n\) with continuous
first derivative \(\symbf{T}'\) allows us to change the variables of
integration from \(\symbf{x}\) to \(\symbf{u}\):
\begin{equation*}
    \int_{R} f\left( \symbf{x} \right) \odif{\symbf{x}} = \int_{R'} f\left( \symbf{T}\left( \symbf{u} \right) \right) \abs*{\det{\left( \symbf{J} \right)}} \odif{\symbf{u}}
\end{equation*}
where \(\symbf{J}\) is the Jacobian matrix of \(\symbf{T}\).
\subsubsection*{Measure Zero Integrals}
If \(\mu\left( R \right) = \int_R \odif{\symbf{x}} = 0\), then
\begin{equation*}
    \int_R f\left( \symbf{x} \right) \odif{\symbf{x}} = 0
\end{equation*}
\subsubsection*{Almost Equal Functions}
If \(f\) and \(g\) are integrable functions, and \(f\left( \symbf{x}
\right) = g\left( \symbf{x} \right)\) for all \(\symbf{x} \in \R^n\)
except for a set of measure zero, then
\begin{equation*}
    \int_{\R^n} f\left( \symbf{x} \right) \odif{\symbf{x}} = \int_{\R^n} g\left( \symbf{x} \right) \odif{\symbf{x}}
\end{equation*}
\end{document}
